# è®­ç»ƒè„šæœ¬ä¿®æ”¹æ€»ç»“

## ğŸ“‹ ä¿®æ”¹æ¦‚è¿°

å·²å°† `scripts/04_train_model.py` å®Œå…¨é€‚é… v2.0 æ¨¡å‹å’Œæ•°æ®æ ¼å¼ã€‚

---

## âœ… ä¸»è¦ä¿®æ”¹

### 1. æ¨¡å‹å¯¼å…¥
```python
# v1.0ï¼ˆæ—§ï¼‰
from models.e3_gnn_encoder import RNAPocketEncoder

# v2.0ï¼ˆæ–°ï¼‰
from models.e3_gnn_encoder_v2 import RNAPocketEncoderV2
from scripts.amber_vocabulary import get_global_encoder
```

---

### 2. æ•°æ®éªŒè¯
æ–°å¢ `LigandEmbeddingDataset` æ•°æ®æ ¼å¼éªŒè¯ï¼š

```python
class LigandEmbeddingDataset:
    def __init__(self, ..., validate_format=True):
        # æ£€æŸ¥ç‰¹å¾ç»´åº¦ï¼ˆåº”ä¸º 4ï¼‰
        if data.x.shape[1] != 4:
            warnings.append("Expected 4D features, got {}D".format(data.x.shape[1]))

        # æ£€æŸ¥å¤šè·³ç´¢å¼•
        if not hasattr(data, 'triple_index'):
            warnings.append("Missing triple_index")
```

**åŠŸèƒ½**:
- âœ… è‡ªåŠ¨æ£€æµ‹æ—§æ ¼å¼æ•°æ®
- âœ… æç¤ºç”¨æˆ·é‡æ–°ç”Ÿæˆæ•°æ®
- âœ… è¿‡æ»¤æ— æ•ˆæ ·æœ¬

---

### 3. æ¨¡å‹åˆå§‹åŒ–

#### v1.0ï¼ˆæ—§ï¼‰
```python
model = RNAPocketEncoder(
    input_dim=11,  # å›ºå®šç»´åº¦
    hidden_irreps="32x0e + 16x1o + 8x2e",
    output_dim=1536
)
```

#### v2.0ï¼ˆæ–°ï¼‰
```python
# è·å–è¯æ±‡è¡¨å¤§å°
encoder = get_global_encoder()

model = RNAPocketEncoderV2(
    num_atom_types=encoder.num_atom_types,      # åŠ¨æ€è·å–
    num_residues=encoder.num_residues,          # åŠ¨æ€è·å–
    atom_embed_dim=32,                          # æ–°å‚æ•°
    residue_embed_dim=16,                       # æ–°å‚æ•°
    hidden_irreps="32x0e + 16x1o + 8x2e",
    output_dim=1536,
    use_multi_hop=True,                         # æ–°å‚æ•°
    use_nonbonded=True,                         # æ–°å‚æ•°
    pooling_type='attention',                   # æ–°å‚æ•°
    dropout=0.0                                 # æ–°å‚æ•°
)
```

**å…³é”®å˜åŒ–**:
- âŒ ç§»é™¤ `input_dim`
- âœ… æ·»åŠ  `num_atom_types`, `num_residues`ï¼ˆä»è¯æ±‡è¡¨è·å–ï¼‰
- âœ… æ·»åŠ  `atom_embed_dim`, `residue_embed_dim`
- âœ… æ·»åŠ å¤šè·³å’Œéé”®æ§åˆ¶å‚æ•°

---

### 4. å‘½ä»¤è¡Œå‚æ•°

#### æ–°å¢å‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|-----|------|--------|------|
| `--atom_embed_dim` | int | 32 | åŸå­ç±»å‹åµŒå…¥ç»´åº¦ |
| `--residue_embed_dim` | int | 16 | æ®‹åŸºåµŒå…¥ç»´åº¦ |
| `--use_multi_hop` | flag | True | å¯ç”¨å¤šè·³æ¶ˆæ¯ä¼ é€’ |
| `--use_nonbonded` | flag | True | å¯ç”¨éé”®äº¤äº’ |
| `--use_gate` | flag | False | ä½¿ç”¨ gate æ¿€æ´» |
| `--use_layer_norm` | flag | False | ä½¿ç”¨å±‚å½’ä¸€åŒ– |
| `--pooling_type` | str | attention | æ± åŒ–ç±»å‹ |
| `--dropout` | float | 0.0 | Dropout ç‡ |
| `--optimizer` | str | adam | ä¼˜åŒ–å™¨ç±»å‹ |
| `--scheduler` | str | plateau | å­¦ä¹ ç‡è°ƒåº¦å™¨ |
| `--grad_clip` | float | 1.0 | æ¢¯åº¦è£å‰ª |

#### ç§»é™¤å‚æ•°

| å‚æ•° | åŸå›  |
|-----|------|
| `--input_dim` | v2.0 ä½¿ç”¨ embeddingï¼Œç»´åº¦åŠ¨æ€ç¡®å®š |

---

### 5. è®­ç»ƒå¾ªç¯å¢å¼º

#### å¯å­¦ä¹ æƒé‡ç›‘æ§

```python
def train_epoch(model, loader, optimizer, device):
    ...
    # è¿”å›å­—å…¸è€Œä¸æ˜¯å•ä¸ªå€¼
    metrics = {'loss': total_loss / num_batches}

    # æ·»åŠ å¯å­¦ä¹ æƒé‡
    if hasattr(model, 'angle_weight'):
        metrics['angle_weight'] = model.angle_weight.item()
    if hasattr(model, 'dihedral_weight'):
        metrics['dihedral_weight'] = model.dihedral_weight.item()
    if hasattr(model, 'nonbonded_weight'):
        metrics['nonbonded_weight'] = model.nonbonded_weight.item()

    return metrics
```

**å®æ—¶è¾“å‡º**:
```
Train Loss: 0.234567
  Angle weight: 0.5234
  Dihedral weight: 0.3123
  Nonbonded weight: 0.2456
```

---

#### æƒé‡å†å²è®°å½•

```python
# è®­ç»ƒå¾ªç¯ä¸­
weight_history = {
    'angle_weight': [],
    'dihedral_weight': [],
    'nonbonded_weight': []
}

# æ¯è½®è®°å½•
if 'angle_weight' in train_metrics:
    weight_history['angle_weight'].append(train_metrics['angle_weight'])

# ä¿å­˜åˆ°å†å²æ–‡ä»¶
history = {
    'train_loss': train_history,
    'val_loss': val_history,
    'learnable_weights': weight_history,  # æ–°å¢
    'config': vars(args)                  # æ–°å¢
}
```

---

### 6. æ¢¯åº¦è£å‰ª

```python
# è®­ç»ƒå¾ªç¯ä¸­æ·»åŠ 
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=args.grad_clip)
```

**ä½œç”¨**: é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§

---

### 7. ä¼˜åŒ–å™¨é€‰æ‹©

```python
if args.optimizer == "adamw":
    optimizer = AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
else:
    optimizer = Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
```

**é€‰é¡¹**:
- **Adam**: æ ‡å‡†é€‰æ‹©
- **AdamW**: æ›´å¥½çš„æƒé‡è¡°å‡ï¼ˆæ¨èï¼‰

---

### 8. å­¦ä¹ ç‡è°ƒåº¦å™¨

```python
if args.scheduler == "cosine":
    scheduler = CosineAnnealingLR(optimizer, T_max=args.num_epochs, eta_min=args.lr * 0.01)
else:
    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)
```

**é€‰é¡¹**:
- **Plateau**: éªŒè¯æŸå¤±ä¸ä¸‹é™æ—¶é™ä½å­¦ä¹ ç‡ï¼ˆæ¨èï¼‰
- **Cosine**: ä½™å¼¦é€€ç«

---

## ğŸ“Š è¾“å‡ºå˜åŒ–

### v1.0 è¾“å‡º
```
models/checkpoints/
â”œâ”€â”€ config.json
â”œâ”€â”€ best_model.pt
â””â”€â”€ training_history.json  # ä»…åŒ…å« train_loss, val_loss
```

### v2.0 è¾“å‡º
```
models/checkpoints_v2/
â”œâ”€â”€ config.json
â”œâ”€â”€ best_model.pt
â”œâ”€â”€ checkpoint_epoch_5.pt
â”œâ”€â”€ checkpoint_epoch_10.pt
â””â”€â”€ training_history.json  # åŒ…å« learnable_weights
```

**`training_history.json` ç¤ºä¾‹**:
```json
{
  "train_loss": [0.5, 0.4, ...],
  "val_loss": [0.45, 0.35, ...],
  "learnable_weights": {
    "angle_weight": [0.5, 0.51, 0.52, ...],
    "dihedral_weight": [0.3, 0.31, 0.29, ...],
    "nonbonded_weight": [0.2, 0.19, 0.21, ...]
  },
  "config": {
    "use_multi_hop": true,
    "use_nonbonded": true,
    ...
  }
}
```

---

## ğŸ”„ å‘åå…¼å®¹æ€§

**ä¸å…¼å®¹**:
- âŒ æ— æ³•ç›´æ¥åŠ è½½ v1.0 çš„æ£€æŸ¥ç‚¹
- âŒ éœ€è¦ v2.0 æ ¼å¼çš„æ•°æ®ï¼ˆ4ç»´ç‰¹å¾ï¼‰

**è¿ç§»æ­¥éª¤**:
1. é‡æ–°ç”Ÿæˆæ•°æ®: `python scripts/03_build_dataset.py`
2. ä½¿ç”¨æ–°è„šæœ¬è®­ç»ƒ: `python scripts/04_train_model.py --use_multi_hop --use_nonbonded`

---

## ğŸš€ ä½¿ç”¨ç¤ºä¾‹

### å¿«é€Ÿå¼€å§‹
```bash
python scripts/04_train_model.py \
    --graph_dir data/processed/graphs \
    --embeddings_path data/processed/ligand_embeddings.h5 \
    --output_dir models/checkpoints_v2 \
    --batch_size 4 \
    --num_epochs 100 \
    --use_multi_hop \
    --use_nonbonded
```

### å®Œæ•´é…ç½®
```bash
python scripts/04_train_model.py \
    --graph_dir data/processed/graphs \
    --embeddings_path data/processed/ligand_embeddings.h5 \
    --output_dir models/checkpoints_full \
    --batch_size 2 \
    --num_epochs 300 \
    --lr 1e-4 \
    --optimizer adamw \
    --scheduler cosine \
    --grad_clip 1.0 \
    --use_multi_hop \
    --use_nonbonded \
    --hidden_irreps "64x0e + 32x1o + 16x2e" \
    --num_layers 5 \
    --dropout 0.1 \
    --pooling_type attention
```

### Baselineï¼ˆä»… 1-hopï¼‰
```bash
python scripts/04_train_model.py \
    --output_dir models/baseline \
    --batch_size 4 \
    --num_epochs 100
# æ³¨æ„ï¼šä¸æ·»åŠ  --use_multi_hop å’Œ --use_nonbonded
```

---

## âš ï¸ å¸¸è§é—®é¢˜

### æ•°æ®æ ¼å¼ä¸åŒ¹é…

**é”™è¯¯**:
```
Expected 4D features, got 11D
```

**è§£å†³**:
```bash
python scripts/03_build_dataset.py
```

---

### å†…å­˜ä¸è¶³

**é”™è¯¯**:
```
CUDA out of memory
```

**è§£å†³**:
1. å‡å° batch_size: `--batch_size 1`
2. å‡å°æ¨¡å‹: `--hidden_irreps "16x0e + 8x1o" --num_layers 2`
3. ç¦ç”¨éé”®: ç§»é™¤ `--use_nonbonded`

---

### è®­ç»ƒä¸ç¨³å®š

**ç—‡çŠ¶**: æŸå¤±éœ‡è¡æˆ– NaN

**è§£å†³**:
1. é™ä½å­¦ä¹ ç‡: `--lr 5e-5`
2. å¢åŠ æ¢¯åº¦è£å‰ª: `--grad_clip 0.5`
3. ä½¿ç”¨ AdamW: `--optimizer adamw`

---

## ğŸ“š æ–‡æ¡£

è¯¦ç»†æ–‡æ¡£è¯·å‚è€ƒ:
- **ä½¿ç”¨æŒ‡å—**: `TRAINING_GUIDE_V2.md`
- **æ¨¡å‹æ–‡æ¡£**: `MODELS_V2_SUMMARY.md`
- **å¤šè·³å®ç°**: `MULTI_HOP_IMPLEMENTATION.md`

---

**ä¿®æ”¹æ—¥æœŸ**: 2025-10-25
**ç‰ˆæœ¬**: v2.0
**çŠ¶æ€**: âœ… å·²æµ‹è¯•ï¼Œå¯ç”¨
