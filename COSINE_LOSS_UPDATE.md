# Cosine Lossæ›´æ–°è¯´æ˜

**æ—¥æœŸ**: 2025-11-02
**ä¿®æ”¹æ–‡ä»¶**: `scripts/04_train_model.py`
**ç›®çš„**: å°†æŸå¤±å‡½æ•°ä»MSEæ›¿æ¢ä¸ºCosine Similarity Loss

---

## âœ… å·²å®Œæˆçš„ä¿®æ”¹

### 1. è®­ç»ƒå‡½æ•° (train)

**ä¿®æ”¹ä½ç½®**: ç¬¬179-182è¡Œ

**ä¿®æ”¹å‰**:
```python
# MSE loss
loss = F.mse_loss(pocket_embedding, target_embedding)
```

**ä¿®æ”¹å**:
```python
# Cosine Similarity Loss (1 - cosine_similarity)
# We want to maximize cosine similarity, so minimize (1 - cosine_similarity)
cosine_sim = F.cosine_similarity(pocket_embedding, target_embedding, dim=1)
loss = (1 - cosine_sim).mean()
```

---

### 2. è¯„ä¼°å‡½æ•° (evaluate)

**ä¿®æ”¹ä½ç½®**: ç¬¬232-275è¡Œ

**ä¸»è¦å˜åŒ–**:
- **ä¸»è¦æŒ‡æ ‡**: ä»MSEæ”¹ä¸ºCosine Loss
- **æ–°å¢æŒ‡æ ‡**: å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦ (avg_cosine_similarity)
- **ä¿ç•™æŒ‡æ ‡**: MSE (ä»…ç”¨äºå¯¹æ¯”)

**è¿”å›å€¼**:
```python
return {
    'cosine_loss': ...,          # ä¸»è¦æŸå¤± (æ–°)
    'avg_cosine_similarity': ..., # æ–°å¢æŒ‡æ ‡
    'mse_loss': ...              # ä¿ç•™ç”¨äºå¯¹æ¯”
}
```

---

### 3. éªŒè¯æ—¥å¿—è¾“å‡º

**ä¿®æ”¹ä½ç½®**: ç¬¬665-668è¡Œ

**ä¿®æ”¹å‰**:
```python
val_loss = val_metrics['mse_loss']
print(f"Val Loss: {val_loss:.6f}, Val L1: {val_metrics['l1_loss']:.6f}")
```

**ä¿®æ”¹å**:
```python
val_loss = val_metrics['cosine_loss']
val_cosine_sim = val_metrics['avg_cosine_similarity']
val_mse = val_metrics['mse_loss']
print(f"Val Cosine Loss: {val_loss:.6f}, Val Cosine Sim: {val_cosine_sim:.4f}, Val MSE: {val_mse:.4f}")
```

**æ–°çš„è¾“å‡ºç¤ºä¾‹**:
```
Val Cosine Loss: 0.2341, Val Cosine Sim: 0.7659, Val MSE: 0.4682
```

---

### 4. è®­ç»ƒå†å²è®°å½•

**ä¿®æ”¹ä½ç½®**: å¤šå¤„

**æ–°å¢è®°å½•**:
- åˆå§‹åŒ–æ—¶æ·»åŠ  `cosine_sim_history = []` (ç¬¬599è¡Œ)
- æ¯ä¸ªepochè®°å½• `cosine_sim_history.append(val_cosine_sim)` (ç¬¬682è¡Œ)
- ä¿å­˜æ—¶åŒ…å« `'val_cosine_similarity': cosine_sim_history` (ç¬¬739è¡Œ)
- æ¢å¤æ—¶åŠ è½½ `cosine_sim_history` (ç¬¬622è¡Œ)

**ä¿å­˜çš„JSONç»“æ„**:
```json
{
  "train_loss": [...],
  "val_loss": [...],
  "val_cosine_similarity": [...],  // æ–°å¢
  "learnable_weights": {...},
  "config": {...}
}
```

---

### 5. æœ€ç»ˆè¾“å‡º

**ä¿®æ”¹ä½ç½®**: ç¬¬747-749è¡Œ

**ä¿®æ”¹å‰**:
```python
print(f"Best validation loss: {best_val_loss:.6f}")
```

**ä¿®æ”¹å**:
```python
print(f"Best validation cosine loss: {best_val_loss:.6f}")
if cosine_sim_history:
    print(f"Best validation cosine similarity: {max(cosine_sim_history):.4f}")
```

---

## ğŸ“Š ç†è§£æ–°æŒ‡æ ‡

### Cosine Loss vs Cosine Similarity

| æŒ‡æ ‡ | èŒƒå›´ | ä¼˜åŒ–ç›®æ ‡ | å«ä¹‰ |
|------|------|----------|------|
| **Cosine Loss** | [0, 2] | æœ€å°åŒ– | 1 - cosine_similarity |
| **Cosine Similarity** | [-1, 1] | æœ€å¤§åŒ– | ç›´æ¥ç›¸ä¼¼åº¦ |

**å…³ç³»**:
```python
cosine_loss = 1 - cosine_similarity

# ç¤ºä¾‹
cosine_similarity = 0.8
cosine_loss = 1 - 0.8 = 0.2  âœ“ (lossè¶Šå°è¶Šå¥½)
```

### è¯„ä¼°æ ‡å‡†

**Cosine Loss** (è¶Šå°è¶Šå¥½):
- `< 0.15`: ä¼˜ç§€ (cosine_sim > 0.85)
- `0.15-0.30`: è‰¯å¥½ (cosine_sim > 0.70)
- `0.30-0.50`: ä¸­ç­‰ (cosine_sim > 0.50)
- `â‰ˆ 1.0`: éšæœºåŸºçº¿ (cosine_sim â‰ˆ 0)
- `> 1.5`: å¾ˆå·®

**Cosine Similarity** (è¶Šå¤§è¶Šå¥½):
- `> 0.85`: ä¼˜ç§€
- `0.70-0.85`: è‰¯å¥½
- `0.50-0.70`: ä¸­ç­‰
- `â‰ˆ 0.0`: éšæœºåŸºçº¿
- `< 0`: å¼‚å¸¸ (å­¦ä¹ é”™è¯¯æ–¹å‘)

---

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### è®­ç»ƒå‘½ä»¤ï¼ˆæ— éœ€ä¿®æ”¹ï¼‰

```bash
python scripts/04_train_model.py \
    --embeddings_path data/processed/ligand_embeddings_256d.h5 \
    --output_dim 256 \
    --batch_size 32 \
    --num_epochs 300 \
    --lr 1e-3
```

### ç›‘æ§è®­ç»ƒ

**å…³æ³¨çš„æŒ‡æ ‡**:
1. **Val Cosine Sim** (ä¸»è¦æŒ‡æ ‡) - åº”è¯¥æŒç»­ä¸Šå‡
2. **Val Cosine Loss** (ä¼˜åŒ–ç›®æ ‡) - åº”è¯¥æŒç»­ä¸‹é™
3. **Val MSE** (å‚è€ƒ) - é€šå¸¸ä¹Ÿä¼šä¸‹é™

**æœŸæœ›çš„è®­ç»ƒè½¨è¿¹**:
```
Epoch 1:   Cosine Sim: 0.05, Cosine Loss: 0.95, MSE: 1.90
Epoch 50:  Cosine Sim: 0.55, Cosine Loss: 0.45, MSE: 0.90
Epoch 150: Cosine Sim: 0.75, Cosine Loss: 0.25, MSE: 0.50
Epoch 300: Cosine Sim: 0.85, Cosine Loss: 0.15, MSE: 0.30
```

---

## ğŸ” éªŒè¯ä¿®æ”¹

### å¿«é€Ÿæµ‹è¯•

è¿è¡Œ1ä¸ªepochéªŒè¯ä»£ç æ˜¯å¦æ­£å¸¸ï¼š

```bash
python scripts/04_train_model.py \
    --num_epochs 1 \
    --batch_size 4 \
    --output_dir test_cosine_loss
```

**é¢„æœŸè¾“å‡º**:
```
Epoch 1/1
--------------------------------------------------
Train Loss: 0.xxxx
Val Cosine Loss: 0.xxxx, Val Cosine Sim: 0.xxxx, Val MSE: x.xxxx
Learning Rate: 1.00e-03
```

---

## ğŸ“ˆ ä¸MSEçš„å¯¹æ¯”

### ç†è®ºæœŸæœ›

| è®­ç»ƒé˜¶æ®µ | MSE Loss | Cosine Loss | Cosine Sim |
|----------|----------|-------------|------------|
| éšæœºåˆå§‹åŒ– | â‰ˆ 2.0 | â‰ˆ 1.0 | â‰ˆ 0.0 |
| è®­ç»ƒä¸­æœŸ | â‰ˆ 0.8 | â‰ˆ 0.4 | â‰ˆ 0.6 |
| è®­ç»ƒåæœŸ | â‰ˆ 0.3 | â‰ˆ 0.15 | â‰ˆ 0.85 |

### é¢„æœŸæ”¹è¿›

ä½¿ç”¨Cosine Lossåº”è¯¥çœ‹åˆ°ï¼š
- âœ… **Cosine Similarityç›´æ¥æå‡**: +5-10%
- âœ… **è®­ç»ƒ-è¯„ä¼°ä¸€è‡´æ€§**: ä¼˜åŒ–ç›®æ ‡ = è¯„ä¼°æŒ‡æ ‡
- âœ… **æ£€ç´¢æ€§èƒ½æå‡**: ä¸‹æ¸¸ä»»åŠ¡è¡¨ç°æ›´å¥½
- âœ… **æ”¶æ•›æ›´ç¨³å®š**: æ–¹å‘å¯¹é½æ¯”è·ç¦»å¯¹é½æ›´é²æ£’

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. Early Stoppingä»ç„¶æœ‰æ•ˆ

è™½ç„¶ä»MSEæ”¹ä¸ºCosine Lossï¼Œä½†early stoppingé€»è¾‘ä»ç„¶æ­£å¸¸å·¥ä½œï¼š
- `val_loss` ç°åœ¨æ˜¯ `cosine_loss`
- æ›´å°çš„cosine_loss = æ›´å¥½çš„æ¨¡å‹ âœ“
- schedulerå’Œbest modelä¿å­˜éƒ½æ­£å¸¸

### 2. MSEä»ç„¶è¢«è®°å½•

ä¿ç•™äº†MSEç”¨äºå¯¹æ¯”ï¼š
- å¯ä»¥éªŒè¯ç†è®ºå…³ç³»: `MSE â‰ˆ 2(1 - cosine_sim)`
- ä¾¿äºä¸ä¹‹å‰çš„MSEæ¨¡å‹å¯¹æ¯”

### 3. å½’ä¸€åŒ–å¾ˆé‡è¦

Cosine Losså‡è®¾åµŒå…¥å·²å½’ä¸€åŒ–ï¼Œç¡®ä¿ï¼š
- Ligand embeddingså·²ç»z-scoreå½’ä¸€åŒ– âœ“
- æ¨¡å‹è¾“å‡ºå¯ä»¥æ˜¯ä»»æ„æ¨¡é•¿ï¼ˆcosineä¼šè‡ªåŠ¨å½’ä¸€åŒ–ï¼‰

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- **ç†è®ºåˆ†æ**: `docs/advanced_loss_functions.md`
- **å¿«é€Ÿå¼€å§‹**: `ADVANCED_LOSS_QUICK_START.md`
- **æŸå¤±èŒƒå›´**: `docs/loss_metric_ranges.md`

---

## ğŸ¯ ä¸‹ä¸€æ­¥å»ºè®®

### 1. åŸºçº¿å¯¹æ¯”ï¼ˆæ¨èï¼‰

è®­ç»ƒä¸¤ä¸ªæ¨¡å‹å¯¹æ¯”ï¼š
```bash
# MSE baseline (å¦‚æœä¹‹å‰è®­ç»ƒè¿‡ï¼Œç›´æ¥ç”¨ç»“æœ)
# æˆ–è€…åˆ‡æ¢å›MSEé‡æ–°è®­ç»ƒ

# Cosine Loss (æ–°)
python scripts/04_train_model.py \
    --output_dir models/checkpoints_cosine_loss
```

å¯¹æ¯”æŒ‡æ ‡ï¼š
- Validation Cosine Similarity (ä¸»è¦)
- Downstream retrieval accuracy (å¦‚æœæœ‰)

### 2. è°ƒä¼˜å­¦ä¹ ç‡ï¼ˆå¯é€‰ï¼‰

Cosine Losså¯èƒ½éœ€è¦ç•¥å¾®è°ƒæ•´å­¦ä¹ ç‡ï¼š
```bash
# å°è¯•ç•¥ä½çš„å­¦ä¹ ç‡
python scripts/04_train_model.py \
    --lr 5e-4  # ä»1e-3é™ä½åˆ°5e-4
```

### 3. ä¸ŠInfoNCEï¼ˆè¿›é˜¶ï¼‰

å¦‚æœCosine Lossæœ‰æå‡ï¼Œè€ƒè™‘å‡çº§åˆ°InfoNCEï¼š
- éœ€è¦batch_size â‰¥ 32
- é¢„æœŸå†æå‡10-20%
- å‚è§ `models/advanced_losses.py`

---

## âœ… æ€»ç»“

**ä¿®æ”¹å®Œæˆ**: âœ… `scripts/04_train_model.py` å·²æ›´æ–°ä¸ºä½¿ç”¨Cosine Loss

**æ ¸å¿ƒå˜åŒ–**:
- è®­ç»ƒæŸå¤±: MSE â†’ Cosine Loss
- ä¸»è¦æŒ‡æ ‡: MSE â†’ Cosine Similarity
- ä¿ç•™å¯¹æ¯”: ä»ç„¶è®°å½•MSE

**æ— éœ€ä¿®æ”¹**:
- å‘½ä»¤è¡Œå‚æ•°
- æ¨¡å‹ç»“æ„
- æ•°æ®åŠ è½½
- Early stoppingé€»è¾‘

**ç«‹å³å¼€å§‹è®­ç»ƒ**:
```bash
python scripts/04_train_model.py --batch_size 32 --num_epochs 300
```

é¢„æœŸçœ‹åˆ°cosine similarityæŒç»­ä¸Šå‡ï¼Œæœ€ç»ˆè¾¾åˆ°0.8+ï¼ğŸš€
