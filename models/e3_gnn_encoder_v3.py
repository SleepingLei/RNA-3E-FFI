#!/usr/bin/env python3
"""
E(3) Equivariant GNN Encoder - Version 3.0 (Improved)

æ”¹è¿›ç‰ˆæœ¬ï¼Œé›†æˆä»¥ä¸‹å¢å¼ºåŠŸèƒ½:
1. âœ… å‡ ä½•ä¿¡æ¯èå…¥çš„è§’åº¦/äºŒé¢è§’æ¶ˆæ¯ä¼ é€’
2. âœ… æ›´ä¸°å¯Œçš„ä¸å˜ç‰¹å¾æå– (56 â†’ 204 ç»´)
3. âœ… Multi-head attention pooling
4. âœ… ç‰©ç†çº¦æŸlossæ”¯æŒ
5. âœ… Bessel basis + Polynomial cutoff (NEW!)
6. âœ… Improved message passing from layers/ (NEW!)
7. âœ… Affine LayerNorm with learnable parameters (NEW!)

ä½¿ç”¨æ–¹æ³•:
    from models.e3_gnn_encoder_v3 import RNAPocketEncoderV3

    model = RNAPocketEncoderV3(
        output_dim=512,
        num_layers=4,
        use_geometric_mp=True,
        use_enhanced_invariants=True,
        use_improved_layers=True,  # ä½¿ç”¨ layers/ æ”¹è¿›ç»„ä»¶
    )
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import MessagePassing
from torch_geometric.utils import softmax, scatter
from e3nn import o3
from e3nn.nn import Gate
import warnings

# Setup path
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent))

# Priority 1: Import improved layers (æœ€ä¼˜å…ˆ)
try:
    from .layers import (
        ImprovedE3MessagePassingLayer,
        BesselBasis,
        PolynomialCutoff,
        EquivariantLayerNorm as LayersEquivariantLayerNorm,
        EquivariantRMSNorm
    )
    _has_improved_layers = True
except ImportError:
    try:
        from layers import (
            ImprovedE3MessagePassingLayer,
            BesselBasis,
            PolynomialCutoff,
            EquivariantLayerNorm as LayersEquivariantLayerNorm,
            EquivariantRMSNorm
        )
        _has_improved_layers = True
    except ImportError:
        _has_improved_layers = False
        ImprovedE3MessagePassingLayer = None
        warnings.warn("layers/ module not found. Using basic implementations.")

# Priority 2: Import V2 base components (å¤‡ç”¨)
try:
    from e3_gnn_encoder_v2 import PhysicalFeatureEmbedding
    _has_v2_components = True
except ImportError:
    _has_v2_components = False
    warnings.warn("Could not import from e3_gnn_encoder_v2.")

# Priority 3: Import improved components (å‡ ä½•MPç­‰)
try:
    from improved_components import (
        GeometricAngleMessagePassing,
        GeometricDihedralMessagePassing,
        EnhancedInvariantExtractor,
        MultiHeadAttentionPooling,
        PhysicsConstraintLoss
    )
    _has_improved_components = True
except ImportError:
    _has_improved_components = False
    warnings.warn("Could not import improved_components.")


class EquivariantLayerNorm(nn.Module):
    """
    E(3)-equivariant LayerNorm - æ”¹è¿›ç‰ˆï¼Œå½’ä¸€åŒ–æ‰€æœ‰ç‰¹å¾ç±»å‹

    ç­–ç•¥:
    - æ ‡é‡ç‰¹å¾ (l=0): ä½¿ç”¨æ ‡å‡† LayerNorm with affine parameters
    - å‘é‡ç‰¹å¾ (l=1): å½’ä¸€åŒ–æ¯ä¸ªå‘é‡çš„èŒƒæ•°
    - å¼ é‡ç‰¹å¾ (l=2): å½’ä¸€åŒ–æ¯ä¸ªå¼ é‡çš„èŒƒæ•°

    è¿™æ ·æ—¢ä¿æŒäº†E(3)ç­‰å˜æ€§ï¼Œåˆé˜²æ­¢äº†ç‰¹å¾å¹…å€¼çˆ†ç‚¸

    æ”¹è¿›: æ·»åŠ å¯å­¦ä¹ çš„ affine å‚æ•° (scale/shift)
    """
    def __init__(self, irreps, normalize_vectors=True, normalize_tensors=True,
                 affine=True, eps=1e-5):
        super().__init__()
        self.irreps = o3.Irreps(irreps)
        self.normalize_vectors = normalize_vectors
        self.normalize_tensors = normalize_tensors
        self.affine = affine
        self.eps = eps

        # æ‰¾åˆ°æ‰€æœ‰ç‰¹å¾çš„ä½ç½®
        self.scalar_indices = []
        self.vector_slices = []  # [(start, end), ...]
        self.tensor_slices = []  # [(start, end), ...]

        idx = 0
        for mul, ir in self.irreps:
            if ir.l == 0:
                # æ ‡é‡ç‰¹å¾
                for _ in range(mul):
                    self.scalar_indices.append(idx)
                    idx += ir.dim
            elif ir.l == 1:
                # å‘é‡ç‰¹å¾ (3D)
                for _ in range(mul):
                    self.vector_slices.append((idx, idx + ir.dim))
                    idx += ir.dim
            elif ir.l == 2:
                # å¼ é‡ç‰¹å¾ (5D)
                for _ in range(mul):
                    self.tensor_slices.append((idx, idx + ir.dim))
                    idx += ir.dim
            else:
                idx += mul * ir.dim

        # ä¸ºæ ‡é‡ç‰¹å¾åˆ›å»º LayerNorm (affine=Falseï¼Œæˆ‘ä»¬è‡ªå·±ç®¡ç†)
        if len(self.scalar_indices) > 0:
            self.layer_norm = nn.LayerNorm(len(self.scalar_indices), elementwise_affine=False, eps=eps)

            # æ·»åŠ å¯å­¦ä¹ çš„ affine å‚æ•°
            if affine:
                self.weight = nn.Parameter(torch.ones(len(self.scalar_indices)))
                self.bias = nn.Parameter(torch.zeros(len(self.scalar_indices)))
            else:
                self.register_parameter('weight', None)
                self.register_parameter('bias', None)
        else:
            self.layer_norm = None
            self.weight = None
            self.bias = None

    def forward(self, x):
        """
        Args:
            x: [num_atoms, irreps_dim]

        Returns:
            x_norm: [num_atoms, irreps_dim] - æ‰€æœ‰ç‰¹å¾éƒ½å½’ä¸€åŒ–
        """
        x_norm = x.clone()

        # 1. å½’ä¸€åŒ–æ ‡é‡ç‰¹å¾
        if self.layer_norm is not None and len(self.scalar_indices) > 0:
            scalar_features = x[:, self.scalar_indices]  # [num_atoms, num_scalars]
            scalar_features_norm = self.layer_norm(scalar_features)

            # åº”ç”¨ affine å˜æ¢ (å¦‚æœå¯ç”¨)
            if self.affine and self.weight is not None:
                scalar_features_norm = scalar_features_norm * self.weight + self.bias

            x_norm[:, self.scalar_indices] = scalar_features_norm

        # 2. å½’ä¸€åŒ–å‘é‡èŒƒæ•° (ä¿æŒæ–¹å‘ï¼Œç¼©æ”¾å¹…å€¼)
        if self.normalize_vectors and len(self.vector_slices) > 0:
            for start, end in self.vector_slices:
                vec = x[:, start:end]  # [num_atoms, 3]
                norm = torch.linalg.norm(vec, dim=-1, keepdim=True).clamp(min=1e-6)
                # å½’ä¸€åŒ–åˆ°å•ä½èŒƒæ•°ï¼Œç„¶åä¹˜ä»¥å¯å­¦ä¹ çš„ç¼©æ”¾å› å­
                # è¿™é‡Œä½¿ç”¨å‡å€¼èŒƒæ•°ä½œä¸ºç›®æ ‡
                mean_norm = norm.mean()
                vec_normalized = vec / norm * mean_norm
                x_norm[:, start:end] = vec_normalized

        # 3. å½’ä¸€åŒ–å¼ é‡èŒƒæ•° (ä¿æŒæ–¹å‘ï¼Œç¼©æ”¾å¹…å€¼)
        if self.normalize_tensors and len(self.tensor_slices) > 0:
            for start, end in self.tensor_slices:
                tensor = x[:, start:end]  # [num_atoms, 5]
                norm = torch.linalg.norm(tensor, dim=-1, keepdim=True).clamp(min=1e-6)
                mean_norm = norm.mean()
                tensor_normalized = tensor / norm * mean_norm
                x_norm[:, start:end] = tensor_normalized

        return x_norm


class RNAPocketEncoderV3(nn.Module):
    """
    E(3) Equivariant GNN for RNA binding pockets - Version 3.0 (Improved)

    ä¸»è¦æ”¹è¿›:
    1. å‡ ä½•å¢å¼ºçš„è§’åº¦/äºŒé¢è§’æ¶ˆæ¯ä¼ é€’
    2. æ›´ä¸°å¯Œçš„ä¸å˜ç‰¹å¾æå– (204ç»´ vs 56ç»´)
    3. Multi-head attention pooling
    4. ç‰©ç†çº¦æŸlossé›†æˆ

    ç›¸æ¯” V2 çš„ä¼˜åŠ¿:
    - æ›´å‡†ç¡®çš„å‡ ä½•å»ºæ¨¡
    - æ›´å¼ºçš„ç‰¹å¾è¡¨è¾¾èƒ½åŠ›
    - æ›´å¥½çš„å›¾çº§åˆ«è¡¨ç¤º
    - ç‰©ç†çº¦æŸæ­£åˆ™åŒ–
    """

    def __init__(
        self,
        input_dim=3,  # [charge, atomic_num, mass]
        feature_hidden_dim=64,
        hidden_irreps="32x0e + 16x1o + 8x2e",
        output_dim=512,
        num_layers=4,
        num_radial_basis=8,
        radial_hidden_dim=64,
        pooling_hidden_dim=128,
        r_max=6.0,
        avg_num_neighbors=None,
        use_gate=True,
        use_layer_norm=False,
        use_multi_hop=True,
        use_nonbonded=True,
        pooling_type='multihead_attention',  # 'multihead_attention' or 'attention'
        num_attention_heads=4,  # For multihead attention
        dropout=0.0,
        # V3æ–°å¢å‚æ•°
        use_geometric_mp=True,  # æ˜¯å¦ä½¿ç”¨å‡ ä½•å¢å¼ºçš„MP
        use_enhanced_invariants=True,  # æ˜¯å¦ä½¿ç”¨å¢å¼ºçš„ä¸å˜é‡æå–
        use_improved_layers=True,  # æ˜¯å¦ä½¿ç”¨ layers/ æ”¹è¿›ç»„ä»¶ (NEW!)
        norm_type='layer',  # 'layer' or 'rms' (NEW!)
        # å¯å­¦ä¹ æƒé‡çš„åˆå§‹å€¼ï¼ˆåœ¨å®é™…æƒé‡ç©ºé—´ï¼Œä¼šè¢«è½¬æ¢åˆ°log-spaceï¼‰
        initial_angle_weight=0.5,
        initial_dihedral_weight=0.5,
        initial_nonbonded_weight=0.5,
    ):
        """
        Args:
            use_geometric_mp: æ˜¯å¦åœ¨è§’åº¦/äºŒé¢è§’MPä¸­ä½¿ç”¨å‡ ä½•ä¿¡æ¯
            use_enhanced_invariants: æ˜¯å¦ä½¿ç”¨å¢å¼ºçš„ä¸å˜é‡æå–(204ç»´ vs 56ç»´)
            use_improved_layers: æ˜¯å¦ä½¿ç”¨ layers/ æ”¹è¿›ç»„ä»¶ (Bessel+Cutoff+ImprovedMP)
            norm_type: å½’ä¸€åŒ–ç±»å‹ ('layer' æˆ– 'rms')
            pooling_type: 'multihead_attention' æˆ– 'attention'
            num_attention_heads: å¤šå¤´æ³¨æ„åŠ›çš„å¤´æ•°
            initial_angle_weight: è§’åº¦æ¶ˆæ¯ä¼ é€’çš„åˆå§‹æƒé‡ (0~1ä¹‹é—´ï¼Œé»˜è®¤0.5)
            initial_dihedral_weight: äºŒé¢è§’æ¶ˆæ¯ä¼ é€’çš„åˆå§‹æƒé‡ (0~1ä¹‹é—´ï¼Œé»˜è®¤0.5)
            initial_nonbonded_weight: éé”®æ¶ˆæ¯ä¼ é€’çš„åˆå§‹æƒé‡ (0~1ä¹‹é—´ï¼Œé»˜è®¤0.5)
        """
        super().__init__()

        self.hidden_irreps = o3.Irreps(hidden_irreps)
        self.output_dim = output_dim
        self.num_layers = num_layers
        self.pooling_type = pooling_type
        self.dropout = dropout
        self.use_multi_hop = use_multi_hop
        self.use_nonbonded = use_nonbonded
        self.use_geometric_mp = use_geometric_mp
        self.use_enhanced_invariants = use_enhanced_invariants
        self.use_improved_layers = use_improved_layers and _has_improved_layers
        self.norm_type = norm_type

        # Learnable combining weights (ä½¿ç”¨ log-space å‚æ•°åŒ–é˜²æ­¢æ— é™å¢é•¿)
        # ä½¿ç”¨ sigmoid çº¦æŸåˆ° [0, 1] èŒƒå›´
        # ä»æƒé‡ç©ºé—´è½¬æ¢åˆ°log-space: logit(w) = log(w / (1-w))
        if use_multi_hop:
            # å°†åˆå§‹æƒé‡ä» [0, 1] è½¬æ¢åˆ° log-space
            # è£å‰ªåˆ° [0.01, 0.99] é¿å…log(0)æˆ–log(âˆ)
            angle_w_clipped = max(0.01, min(0.99, initial_angle_weight))
            dihedral_w_clipped = max(0.01, min(0.99, initial_dihedral_weight))

            angle_logit = torch.log(torch.tensor(angle_w_clipped / (1 - angle_w_clipped)))
            dihedral_logit = torch.log(torch.tensor(dihedral_w_clipped / (1 - dihedral_w_clipped)))

            self.log_angle_weight = nn.Parameter(angle_logit)
            self.log_dihedral_weight = nn.Parameter(dihedral_logit)

        if use_nonbonded:
            nonbonded_w_clipped = max(0.01, min(0.99, initial_nonbonded_weight))
            nonbonded_logit = torch.log(torch.tensor(nonbonded_w_clipped / (1 - nonbonded_w_clipped)))
            self.log_nonbonded_weight = nn.Parameter(nonbonded_logit)

        # Input embedding (same as V2)
        self.input_embedding = PhysicalFeatureEmbedding(
            input_dim=input_dim,
            hidden_dim=feature_hidden_dim,
            output_irreps=hidden_irreps
        )

        # 1-hop bonded message passing (ä½¿ç”¨æ”¹è¿›ç‰ˆæœ¬ if available)
        self.bonded_mp_layers = nn.ModuleList()
        for i in range(num_layers):
            if self.use_improved_layers:
                # ä½¿ç”¨ ImprovedE3MessagePassingLayer from layers/
                layer = ImprovedE3MessagePassingLayer(
                    irreps_in=self.hidden_irreps,
                    irreps_out=self.hidden_irreps,
                    irreps_sh="0e + 1o + 2e",
                    r_max=r_max,
                    num_radial_basis=num_radial_basis,
                    radial_hidden_dim=radial_hidden_dim,
                    avg_num_neighbors=avg_num_neighbors,
                    use_gate=use_gate,
                    use_sc=True,
                    use_resnet=True,
                    use_layer_norm=use_layer_norm,
                    edge_attr_dim=2  # [req, k]
                )
            else:
                # ä½¿ç”¨ V2 çš„åŸºç¡€å®ç°
                from e3_gnn_encoder_v2 import E3GNNMessagePassingLayer
                layer = E3GNNMessagePassingLayer(
                    irreps_in=self.hidden_irreps,
                    irreps_out=self.hidden_irreps,
                    irreps_sh="0e + 1o + 2e",
                    num_radial_basis=num_radial_basis,
                    radial_hidden_dim=radial_hidden_dim,
                    edge_attr_dim=2,  # [req, k]
                    r_max=r_max,
                    avg_num_neighbors=avg_num_neighbors,
                    use_gate=use_gate,
                    use_sc=True,
                    use_resnet=True,
                    use_layer_norm=use_layer_norm
                )
            self.bonded_mp_layers.append(layer)

        # 2-hop angle message passing (IMPROVED with geometry!)
        if use_multi_hop:
            self.angle_mp_layers = nn.ModuleList()
            for i in range(num_layers):
                if use_geometric_mp:
                    # ä½¿ç”¨å‡ ä½•å¢å¼ºç‰ˆæœ¬ï¼ˆå¸¦ LayerNorm ç¨³å®šæ€§æ”¹è¿›ï¼‰
                    layer = GeometricAngleMessagePassing(
                        irreps_in=self.hidden_irreps,
                        irreps_out=self.hidden_irreps,
                        angle_attr_dim=2,
                        hidden_dim=64,
                        use_geometry=True,
                        use_layer_norm=True  # å¯ç”¨ LayerNorm æé«˜æ•°å€¼ç¨³å®šæ€§
                    )
                else:
                    # ä½¿ç”¨åŸå§‹ç‰ˆæœ¬ï¼ˆä»v2å¯¼å…¥ï¼‰
                    from e3_gnn_encoder_v2 import AngleMessagePassing
                    layer = AngleMessagePassing(
                        irreps_in=self.hidden_irreps,
                        irreps_out=self.hidden_irreps,
                        angle_attr_dim=2,
                        hidden_dim=64
                    )
                self.angle_mp_layers.append(layer)

        # 3-hop dihedral message passing (IMPROVED with geometry!)
        if use_multi_hop:
            self.dihedral_mp_layers = nn.ModuleList()
            for i in range(num_layers):
                if use_geometric_mp:
                    # ä½¿ç”¨å‡ ä½•å¢å¼ºç‰ˆæœ¬ï¼ˆå¸¦ LayerNorm ç¨³å®šæ€§æ”¹è¿›ï¼‰
                    layer = GeometricDihedralMessagePassing(
                        irreps_in=self.hidden_irreps,
                        irreps_out=self.hidden_irreps,
                        dihedral_attr_dim=3,
                        hidden_dim=64,
                        use_geometry=True,
                        use_layer_norm=True  # å¯ç”¨ LayerNorm æé«˜æ•°å€¼ç¨³å®šæ€§
                    )
                else:
                    # ä½¿ç”¨åŸå§‹ç‰ˆæœ¬
                    from e3_gnn_encoder_v2 import DihedralMessagePassing
                    layer = DihedralMessagePassing(
                        irreps_in=self.hidden_irreps,
                        irreps_out=self.hidden_irreps,
                        dihedral_attr_dim=3,
                        hidden_dim=64
                    )
                self.dihedral_mp_layers.append(layer)

        # Non-bonded message passing (same as V2)
        if use_nonbonded:
            self.nonbonded_mp_layers = nn.ModuleList()
            for i in range(num_layers):
                layer = E3GNNMessagePassingLayer(
                    irreps_in=self.hidden_irreps,
                    irreps_out=self.hidden_irreps,
                    irreps_sh="0e + 1o + 2e",
                    num_radial_basis=num_radial_basis,
                    radial_hidden_dim=radial_hidden_dim,
                    edge_attr_dim=3,  # [LJ_A, LJ_B, distance]
                    r_max=r_max,
                    avg_num_neighbors=avg_num_neighbors,
                    use_gate=use_gate,
                    use_sc=False,
                    use_resnet=False,
                    use_layer_norm=use_layer_norm
                )
                self.nonbonded_mp_layers.append(layer)

        # LayerNorm for stabilizing multi-hop aggregation (é˜²æ­¢ç‰¹å¾å¹…å€¼çˆ†ç‚¸)
        # æ”¯æŒ RMSNorm (æ›´å¿«) æˆ– LayerNorm (with affine)
        if use_multi_hop or use_nonbonded:
            self.aggregation_layer_norms = nn.ModuleList()
            for i in range(num_layers):
                if self.norm_type == 'rms' and self.use_improved_layers:
                    # ä½¿ç”¨ RMSNorm from layers/ (æ›´å¿«)
                    self.aggregation_layer_norms.append(
                        EquivariantRMSNorm(self.hidden_irreps, affine=True)
                    )
                elif self.use_improved_layers:
                    # ä½¿ç”¨ layers/ çš„ LayerNorm (æœ‰ affine)
                    self.aggregation_layer_norms.append(
                        LayersEquivariantLayerNorm(self.hidden_irreps, affine=True)
                    )
                else:
                    # ä½¿ç”¨æœ¬åœ°çš„ EquivariantLayerNorm (ç°åœ¨ä¹Ÿæœ‰ affine)
                    self.aggregation_layer_norms.append(
                        EquivariantLayerNorm(self.hidden_irreps, affine=True)
                    )

        # Invariant feature extraction (IMPROVED!)
        if use_enhanced_invariants:
            # ä½¿ç”¨å¢å¼ºç‰ˆæœ¬: 204ç»´ï¼ˆå¸¦å½’ä¸€åŒ–ç¨³å®šæ€§æ”¹è¿›ï¼‰
            self.invariant_extractor = EnhancedInvariantExtractor(
                hidden_irreps,
                normalize_features=True  # å¯ç”¨ç‰¹å¾å½’ä¸€åŒ–æé«˜æ•°å€¼ç¨³å®šæ€§
            )
            self.invariant_dim = self.invariant_extractor.invariant_dim  # 204
        else:
            # ä½¿ç”¨åŸå§‹ç‰ˆæœ¬: 56ç»´
            self.invariant_extractor = None
            scalar_irreps = o3.Irreps([(mul, ir) for mul, ir in self.hidden_irreps if ir.l == 0])
            self.scalar_dim = scalar_irreps.dim
            self.num_l1_irreps = sum(mul for mul, ir in self.hidden_irreps if ir.l == 1)
            self.num_l2_irreps = sum(mul for mul, ir in self.hidden_irreps if ir.l == 2)
            self.invariant_dim = self.scalar_dim + self.num_l1_irreps + self.num_l2_irreps  # 56
            self._build_irreps_slices()

        # Pooling (IMPROVED with multi-head attention!)
        if pooling_type == 'multihead_attention':
            self.pooling = MultiHeadAttentionPooling(
                input_dim=self.invariant_dim,
                num_heads=num_attention_heads,
                hidden_dim=pooling_hidden_dim,
                dropout=dropout
            )
            self.pooling_mlp = None
        elif pooling_type == 'attention':
            # åŸå§‹çš„MLP attention
            self.pooling_mlp = nn.Sequential(
                nn.Linear(self.invariant_dim, pooling_hidden_dim),
                nn.SiLU(),
                nn.Dropout(dropout) if dropout > 0 else nn.Identity(),
                nn.Linear(pooling_hidden_dim, pooling_hidden_dim),
                nn.SiLU(),
                nn.Dropout(dropout) if dropout > 0 else nn.Identity(),
                nn.Linear(pooling_hidden_dim, 1)
            )
            self.pooling = None
        else:
            # No attention
            self.pooling_mlp = None
            self.pooling = None

        # Output projection
        self.output_projection = nn.Sequential(
            nn.Linear(self.invariant_dim, output_dim),
            nn.LayerNorm(output_dim)
        )

    def _build_irreps_slices(self):
        """Build index slices (for original invariant extraction)"""
        self.irreps_slices = {'l0': [], 'l1': [], 'l2': []}

        idx = 0
        for mul, ir in self.hidden_irreps:
            dim = ir.dim
            for _ in range(mul):
                if ir.l == 0:
                    self.irreps_slices['l0'].append((idx, idx + dim))
                elif ir.l == 1:
                    self.irreps_slices['l1'].append((idx, idx + dim))
                elif ir.l == 2:
                    self.irreps_slices['l2'].append((idx, idx + dim))
                idx += dim

    @property
    def angle_weight(self):
        """è¿”å›çº¦æŸåçš„è§’åº¦æƒé‡ (èŒƒå›´: [0, 1])"""
        if hasattr(self, 'log_angle_weight'):
            return torch.sigmoid(self.log_angle_weight)
        return torch.tensor(0.0, device=self.log_angle_weight.device if hasattr(self, 'log_angle_weight') else 'cpu')

    @property
    def dihedral_weight(self):
        """è¿”å›çº¦æŸåçš„äºŒé¢è§’æƒé‡ (èŒƒå›´: [0, 1])"""
        if hasattr(self, 'log_dihedral_weight'):
            return torch.sigmoid(self.log_dihedral_weight)
        return torch.tensor(0.0, device=self.log_dihedral_weight.device if hasattr(self, 'log_dihedral_weight') else 'cpu')

    @property
    def nonbonded_weight(self):
        """è¿”å›çº¦æŸåçš„éé”®æƒé‡ (èŒƒå›´: [0, 1])"""
        if hasattr(self, 'log_nonbonded_weight'):
            return torch.sigmoid(self.log_nonbonded_weight)
        return torch.tensor(0.0, device=self.log_nonbonded_weight.device if hasattr(self, 'log_nonbonded_weight') else 'cpu')

    def extract_invariant_features(self, h):
        """Extract E(3) invariant features (original version for V2 compatibility)"""
        if self.invariant_extractor is not None:
            # Use enhanced version
            return self.invariant_extractor(h)

        # Original version (from V2)
        invariant_features = []

        # Scalars
        for start, end in self.irreps_slices['l0']:
            invariant_features.append(h[:, start:end])

        # Vector norms
        for start, end in self.irreps_slices['l1']:
            vec = h[:, start:end]
            norm = torch.linalg.norm(vec, dim=-1, keepdim=True)
            invariant_features.append(norm)

        # Tensor norms
        for start, end in self.irreps_slices['l2']:
            tensor = h[:, start:end]
            norm = torch.linalg.norm(tensor, dim=-1, keepdim=True)
            invariant_features.append(norm)

        t = torch.cat(invariant_features, dim=-1)
        return t

    def forward(self, data):
        """
        Forward pass.

        Args:
            data: PyTorch Geometric Data object

        Returns:
            Pocket embeddings [batch_size, output_dim]
        """
        x, pos, edge_index = data.x, data.pos, data.edge_index
        edge_attr = data.edge_attr if hasattr(data, 'edge_attr') else None

        # Get batch
        if hasattr(data, 'batch') and data.batch is not None:
            batch = data.batch
        else:
            batch = torch.zeros(x.size(0), dtype=torch.long, device=x.device)

        # Initial embedding
        h = self.input_embedding(x)

        # Message passing layers
        for i in range(self.num_layers):
            h_new = h

            # 1-hop bonded
            h_bonded = self.bonded_mp_layers[i](h, pos, edge_index, edge_attr)
            h_new = h_bonded

            # 2-hop angles
            if self.use_multi_hop and hasattr(data, 'triple_index'):
                if self.use_geometric_mp:
                    # ä¼ å…¥posç”¨äºå‡ ä½•è®¡ç®—
                    h_angle = self.angle_mp_layers[i](h, pos, data.triple_index, data.triple_attr)
                else:
                    # åŸå§‹ç‰ˆæœ¬ä¸éœ€è¦pos
                    h_angle = self.angle_mp_layers[i](h, data.triple_index, data.triple_attr)
                h_new = h_new + self.angle_weight * h_angle

            # 3-hop dihedrals
            if self.use_multi_hop and hasattr(data, 'quadra_index'):
                if self.use_geometric_mp:
                    # ä¼ å…¥posç”¨äºå‡ ä½•è®¡ç®—
                    h_dihedral = self.dihedral_mp_layers[i](h, pos, data.quadra_index, data.quadra_attr)
                else:
                    # åŸå§‹ç‰ˆæœ¬ä¸éœ€è¦pos
                    h_dihedral = self.dihedral_mp_layers[i](h, data.quadra_index, data.quadra_attr)
                h_new = h_new + self.dihedral_weight * h_dihedral

            # Non-bonded
            if self.use_nonbonded and hasattr(data, 'nonbonded_edge_index'):
                h_nonbonded = self.nonbonded_mp_layers[i](
                    h, pos, data.nonbonded_edge_index, data.nonbonded_edge_attr
                )
                h_new = h_new + self.nonbonded_weight * h_nonbonded

            # Apply LayerNorm to stabilize aggregated features (é˜²æ­¢å¹…å€¼çˆ†ç‚¸)
            if (self.use_multi_hop or self.use_nonbonded) and hasattr(self, 'aggregation_layer_norms'):
                h = self.aggregation_layer_norms[i](h_new)
            else:
                h = h_new

            # Dropout on scalars only
            if self.dropout > 0 and self.training:
                scalar_mask = torch.zeros(h.size(-1), dtype=torch.bool, device=h.device)
                idx = 0
                for mul, ir in self.hidden_irreps:
                    if ir.l == 0:
                        scalar_mask[idx:idx + mul * ir.dim] = True
                    idx += mul * ir.dim

                h_dropped = h.clone()
                h_dropped[..., scalar_mask] = F.dropout(
                    h[..., scalar_mask],
                    p=self.dropout,
                    training=self.training
                )
                h = h_dropped

        # Extract invariant features
        t = self.extract_invariant_features(h)

        # Pooling
        if self.pooling_type == 'multihead_attention' and self.pooling is not None:
            # Multi-head attention pooling
            graph_embedding = self.pooling(t, batch)
        elif self.pooling_type == 'attention' and self.pooling_mlp is not None:
            # Original MLP attention pooling
            attention_logits = self.pooling_mlp(t)
            attention_weights = softmax(attention_logits, index=batch, dim=0)
            weighted_features = t * attention_weights
            graph_embedding = scatter(
                weighted_features,
                index=batch,
                dim=0,
                reduce='sum'
            )
        elif self.pooling_type == 'mean':
            graph_embedding = scatter(t, index=batch, dim=0, reduce='mean')
        elif self.pooling_type == 'sum':
            graph_embedding = scatter(t, index=batch, dim=0, reduce='sum')
        elif self.pooling_type == 'max':
            graph_embedding = scatter(t, index=batch, dim=0, reduce='max')
        else:
            # Default to mean
            graph_embedding = scatter(t, index=batch, dim=0, reduce='mean')

        # Project to output
        output = self.output_projection(graph_embedding)

        return output

    def get_node_embeddings(self, data):
        """
        Get per-node invariant embeddings (before pooling).

        Useful for visualization and analysis.
        """
        x, pos, edge_index = data.x, data.pos, data.edge_index
        edge_attr = data.edge_attr if hasattr(data, 'edge_attr') else None

        # Initial embedding
        h = self.input_embedding(x)

        # Message passing
        for i in range(self.num_layers):
            h_bonded = self.bonded_mp_layers[i](h, pos, edge_index, edge_attr)
            h_new = h_bonded

            if self.use_multi_hop and hasattr(data, 'triple_index'):
                if self.use_geometric_mp:
                    h_angle = self.angle_mp_layers[i](h, pos, data.triple_index, data.triple_attr)
                else:
                    h_angle = self.angle_mp_layers[i](h, data.triple_index, data.triple_attr)
                h_new = h_new + self.angle_weight * h_angle

            if self.use_multi_hop and hasattr(data, 'quadra_index'):
                if self.use_geometric_mp:
                    h_dihedral = self.dihedral_mp_layers[i](h, pos, data.quadra_index, data.quadra_attr)
                else:
                    h_dihedral = self.dihedral_mp_layers[i](h, data.quadra_index, data.quadra_attr)
                h_new = h_new + self.dihedral_weight * h_dihedral

            if self.use_nonbonded and hasattr(data, 'nonbonded_edge_index'):
                h_nonbonded = self.nonbonded_mp_layers[i](
                    h, pos, data.nonbonded_edge_index, data.nonbonded_edge_attr
                )
                h_new = h_new + self.nonbonded_weight * h_nonbonded

            h = h_new

        # Extract invariant features
        t = self.extract_invariant_features(h)

        return t

    def get_weight_stats(self):
        """
        è·å–å¯å­¦ä¹ æƒé‡çš„ç»Ÿè®¡ä¿¡æ¯ (ç”¨äºç›‘æ§)

        Returns:
            dict: åŒ…å«æƒé‡å€¼ã€æ¢¯åº¦ã€logç©ºé—´å‚æ•°ç­‰ä¿¡æ¯
        """
        stats = {}

        if hasattr(self, 'log_angle_weight'):
            stats['angle_weight'] = self.angle_weight.item()
            stats['log_angle_weight'] = self.log_angle_weight.item()
            if self.log_angle_weight.grad is not None:
                stats['angle_weight_grad'] = self.log_angle_weight.grad.item()

        if hasattr(self, 'log_dihedral_weight'):
            stats['dihedral_weight'] = self.dihedral_weight.item()
            stats['log_dihedral_weight'] = self.log_dihedral_weight.item()
            if self.log_dihedral_weight.grad is not None:
                stats['dihedral_weight_grad'] = self.log_dihedral_weight.grad.item()

        if hasattr(self, 'log_nonbonded_weight'):
            stats['nonbonded_weight'] = self.nonbonded_weight.item()
            stats['log_nonbonded_weight'] = self.log_nonbonded_weight.item()
            if self.log_nonbonded_weight.grad is not None:
                stats['nonbonded_weight_grad'] = self.log_nonbonded_weight.grad.item()

        return stats

    def get_feature_stats(self, data):
        """
        è·å–æ¯å±‚ç‰¹å¾çš„ç»Ÿè®¡ä¿¡æ¯ (ç”¨äºè¯Šæ–­)

        Returns:
            dict: åŒ…å«æ¯å±‚ç‰¹å¾çš„èŒƒæ•°ã€å‡å€¼ã€æ ‡å‡†å·®ç­‰
        """
        x, pos, edge_index = data.x, data.pos, data.edge_index
        edge_attr = data.edge_attr if hasattr(data, 'edge_attr') else None

        stats = {'layers': []}

        # Initial embedding
        h = self.input_embedding(x)
        stats['input_norm'] = torch.linalg.norm(h, dim=-1).mean().item()

        # Message passing
        for i in range(self.num_layers):
            layer_stats = {}

            # 1-hop
            h_bonded = self.bonded_mp_layers[i](h, pos, edge_index, edge_attr)
            layer_stats['bonded_norm'] = torch.linalg.norm(h_bonded, dim=-1).mean().item()
            h_new = h_bonded

            # 2-hop
            if self.use_multi_hop and hasattr(data, 'triple_index'):
                if self.use_geometric_mp:
                    h_angle = self.angle_mp_layers[i](h, pos, data.triple_index, data.triple_attr)
                else:
                    h_angle = self.angle_mp_layers[i](h, data.triple_index, data.triple_attr)
                layer_stats['angle_norm'] = torch.linalg.norm(h_angle, dim=-1).mean().item()
                h_new = h_new + self.angle_weight * h_angle

            # 3-hop
            if self.use_multi_hop and hasattr(data, 'quadra_index'):
                if self.use_geometric_mp:
                    h_dihedral = self.dihedral_mp_layers[i](h, pos, data.quadra_index, data.quadra_attr)
                else:
                    h_dihedral = self.dihedral_mp_layers[i](h, data.quadra_index, data.quadra_attr)
                layer_stats['dihedral_norm'] = torch.linalg.norm(h_dihedral, dim=-1).mean().item()
                h_new = h_new + self.dihedral_weight * h_dihedral

            # Non-bonded
            if self.use_nonbonded and hasattr(data, 'nonbonded_edge_index'):
                h_nonbonded = self.nonbonded_mp_layers[i](
                    h, pos, data.nonbonded_edge_index, data.nonbonded_edge_attr
                )
                layer_stats['nonbonded_norm'] = torch.linalg.norm(h_nonbonded, dim=-1).mean().item()
                h_new = h_new + self.nonbonded_weight * h_nonbonded

            # èšåˆåçš„ç»Ÿè®¡
            layer_stats['aggregated_norm'] = torch.linalg.norm(h_new, dim=-1).mean().item()

            # LayerNormå
            if (self.use_multi_hop or self.use_nonbonded) and hasattr(self, 'aggregation_layer_norms'):
                h = self.aggregation_layer_norms[i](h_new)
                layer_stats['after_norm'] = torch.linalg.norm(h, dim=-1).mean().item()
            else:
                h = h_new

            stats['layers'].append(layer_stats)

        return stats


# ============================================================================
# Test Code
# ============================================================================

if __name__ == "__main__":
    from torch_geometric.data import Data, Batch

    print("=" * 80)
    print("Testing E(3) GNN Encoder V3.0 (Improved)")
    print("=" * 80)

    # Create realistic test data
    num_nodes = 100
    num_edges = 300
    num_angles = 150
    num_dihedrals = 80
    num_nonbonded = 200

    x = torch.randn(num_nodes, 3)  # Pure physical features
    pos = torch.randn(num_nodes, 3)
    edge_index = torch.randint(0, num_nodes, (2, num_edges))
    edge_attr = torch.rand(num_edges, 2) * 2  # [k, req]

    # Angles: [i, j, k]
    triple_index = torch.randint(0, num_nodes, (3, num_angles))
    triple_attr = torch.rand(num_angles, 2)  # [theta_eq, k]
    triple_attr[:, 0] = triple_attr[:, 0] * 3.14  # Convert to radians

    # Dihedrals: [i, j, k, l]
    quadra_index = torch.randint(0, num_nodes, (4, num_dihedrals))
    quadra_attr = torch.rand(num_dihedrals, 3)  # [phi_k, per, phase]
    quadra_attr[:, 1] = torch.randint(1, 4, (num_dihedrals,)).float()  # periodicity
    quadra_attr[:, 2] = quadra_attr[:, 2] * 6.28  # phase in radians

    # Non-bonded
    nonbonded_edge_index = torch.randint(0, num_nodes, (2, num_nonbonded))
    nonbonded_edge_attr = torch.rand(num_nonbonded, 3)  # [LJ_A, LJ_B, dist]

    data = Data(
        x=x,
        pos=pos,
        edge_index=edge_index,
        edge_attr=edge_attr,
        triple_index=triple_index,
        triple_attr=triple_attr,
        quadra_index=quadra_index,
        quadra_attr=quadra_attr,
        nonbonded_edge_index=nonbonded_edge_index,
        nonbonded_edge_attr=nonbonded_edge_attr
    )

    # Test V3 with all improvements
    print("\nTest 1: V3 with all improvements enabled")
    print("-" * 80)
    model_v3 = RNAPocketEncoderV3(
        output_dim=512,
        num_layers=4,
        use_geometric_mp=True,
        use_enhanced_invariants=True,
        pooling_type='multihead_attention',
        num_attention_heads=4
    )

    print(f"Model parameters: {sum(p.numel() for p in model_v3.parameters()):,}")

    # Forward pass
    output = model_v3(data)
    print(f"Input: {num_nodes} atoms")
    print(f"Output: {output.shape}")
    print(f"âœ“ Forward pass successful!")

    # Test node embeddings
    node_emb = model_v3.get_node_embeddings(data)
    print(f"Node embeddings: {node_emb.shape}")
    print(f"Invariant dim: {model_v3.invariant_dim}")

    # Test V3 without improvements (should be similar to V2)
    print("\nTest 2: V3 with improvements disabled (V2-like)")
    print("-" * 80)
    model_v2_like = RNAPocketEncoderV3(
        output_dim=512,
        num_layers=4,
        use_geometric_mp=False,
        use_enhanced_invariants=False,
        pooling_type='attention'
    )

    output2 = model_v2_like(data)
    print(f"Output: {output2.shape}")
    print(f"Invariant dim: {model_v2_like.invariant_dim}")
    print(f"âœ“ V2-compatible mode works!")

    # Test batched data
    print("\nTest 3: Batched data")
    print("-" * 80)
    batch_data = Batch.from_data_list([data, data, data])
    batch_output = model_v3(batch_data)
    print(f"Batch size: 3")
    print(f"Output: {batch_output.shape}")
    print(f"âœ“ Batch processing works!")

    # Test physics loss
    print("\nTest 4: Physics constraint loss")
    print("-" * 80)
    physics_loss_fn = PhysicsConstraintLoss(
        use_bond=True,
        use_angle=True,
        use_dihedral=True
    )

    loss, loss_dict = physics_loss_fn(data)
    print(f"Total physics loss: {loss.item():.4f}")
    for key, val in loss_dict.items():
        print(f"  {key}: {val:.4f}")
    print(f"âœ“ Physics loss works!")

    print("\n" + "=" * 80)
    print("All tests passed! âœ“")
    print("=" * 80)

    # Print summary
    print("\nğŸ“Š Model Comparison Summary:")
    print("-" * 80)
    print(f"{'Feature':<40} {'V2':<15} {'V3':<15}")
    print("-" * 80)
    print(f"{'Geometric Angle/Dihedral MP':<40} {'âŒ No':<15} {'âœ… Yes':<15}")
    print(f"{'Invariant Features Dim':<40} {'56':<15} {'204':<15}")
    print(f"{'Multi-head Attention Pooling':<40} {'âŒ No':<15} {'âœ… Yes':<15}")
    print(f"{'Physics Constraint Loss':<40} {'âŒ No':<15} {'âœ… Yes':<15}")
    print(f"{'Backward Compatible':<40} {'N/A':<15} {'âœ… Yes':<15}")
    print("-" * 80)
