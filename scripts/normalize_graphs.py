#!/usr/bin/env python3
"""
Normalize Existing Graph Features

This script takes existing graph files (.pt format) generated by scripts/03_build_dataset.py
and applies z-score normalization to the node features.

Usage:
    python scripts/normalize_graphs.py --graph-dir data/processed/graphs
    python scripts/normalize_graphs.py --graph-dir data/processed/graphs --output-dir data/processed/graphs_normalized
    python scripts/normalize_graphs.py --graph-dir data/processed/graphs --inplace
"""
import argparse
import numpy as np
import torch
from pathlib import Path
from tqdm import tqdm
import glob


def normalize_graph_features(graph_dir, output_dir=None, params_output_path=None, inplace=False):
    """
    Normalize all graph node features using z-score normalization.

    Args:
        graph_dir: Directory containing .pt graph files
        output_dir: Directory to save normalized graphs (if None and not inplace, uses graph_dir_normalized)
        params_output_path: Path to save normalization parameters
        inplace: If True, overwrite original files

    Returns:
        Tuple of (output_dir, params_path)
    """
    graph_dir = Path(graph_dir)

    if not graph_dir.exists():
        raise FileNotFoundError(f"Graph directory not found: {graph_dir}")

    # Find all graph files
    graph_files = sorted(graph_dir.glob("*.pt"))

    if len(graph_files) == 0:
        raise ValueError(f"No .pt graph files found in {graph_dir}")

    # Determine output paths
    if inplace:
        output_dir = graph_dir
        print(f"⚠️  WARNING: In-place mode enabled. Original files will be overwritten!")
    elif output_dir is None:
        output_dir = graph_dir.parent / f"{graph_dir.name}_normalized"
    else:
        output_dir = Path(output_dir)

    if params_output_path is None:
        params_output_path = graph_dir.parent / "node_feature_norm_params.npz"
    else:
        params_output_path = Path(params_output_path)

    # Ensure output directory exists
    output_dir.mkdir(parents=True, exist_ok=True)
    params_output_path.parent.mkdir(parents=True, exist_ok=True)

    print(f"\n{'='*60}")
    print(f"Normalizing Graph Node Features")
    print(f"{'='*60}\n")
    print(f"Input:  {graph_dir}")
    print(f"Output: {output_dir}")
    print(f"Params: {params_output_path}")
    print(f"Found {len(graph_files)} graph files")
    print()

    # Step 1: Collect all node features
    print("Step 1: Collecting node features from all graphs...")
    all_node_features = []

    for graph_path in tqdm(graph_files, desc="Loading graphs"):
        try:
            data = torch.load(graph_path)
            if hasattr(data, 'x') and data.x is not None:
                all_node_features.append(data.x.numpy())
        except Exception as e:
            print(f"Warning: Failed to load {graph_path}: {e}")

    if len(all_node_features) == 0:
        raise ValueError("No node features collected from graphs")

    print(f"✓ Collected features from {len(all_node_features)} graphs")

    # Step 2: Compute normalization parameters
    print("\nStep 2: Computing normalization parameters...")

    # Concatenate all features
    all_features = np.vstack(all_node_features)
    print(f"Feature matrix shape: {all_features.shape}")
    print(f"  Total atoms:     {all_features.shape[0]}")
    print(f"  Feature dimension: {all_features.shape[1]}")

    # Compute global statistics
    feature_mean = np.mean(all_features, axis=0)
    feature_std = np.std(all_features, axis=0)

    # Add small epsilon to avoid division by zero for constant features
    epsilon = 1e-8
    feature_std = np.where(feature_std < epsilon, 1.0, feature_std)

    print(f"✓ Computed normalization parameters")
    print(f"  Mean range: [{feature_mean.min():.4f}, {feature_mean.max():.4f}]")
    print(f"  Std range:  [{feature_std.min():.4f}, {feature_std.max():.4f}]")

    # Check for potential issues
    constant_dims = np.sum(feature_std == 1.0)
    if constant_dims > 0:
        print(f"  ⚠️  Warning: {constant_dims} dimensions have constant values (std < {epsilon})")

    # Step 3: Save normalization parameters
    print(f"\nStep 3: Saving normalization parameters to {params_output_path}...")
    np.savez(params_output_path,
             mean=feature_mean,
             std=feature_std)
    print(f"✓ Saved normalization parameters")

    # Step 4: Apply normalization to all graphs
    print("\nStep 4: Applying normalization and saving graphs...")

    for graph_path in tqdm(graph_files, desc="Normalizing graphs"):
        try:
            data = torch.load(graph_path)

            if hasattr(data, 'x') and data.x is not None:
                # Apply normalization
                x_normalized = (data.x.numpy() - feature_mean) / feature_std
                data.x = torch.from_numpy(x_normalized.astype(np.float32))

                # Determine output path
                if inplace:
                    output_path = graph_path
                else:
                    output_path = output_dir / graph_path.name

                # Save normalized graph
                torch.save(data, output_path)

        except Exception as e:
            print(f"Warning: Failed to normalize {graph_path}: {e}")

    print(f"✓ Normalized and saved {len(graph_files)} graphs")

    # Step 5: Verification
    print("\nStep 5: Verification...")

    # Load a few normalized graphs to verify
    sample_size = min(10, len(graph_files))
    sample_features = []

    for i, graph_path in enumerate(graph_files[:sample_size]):
        output_path = output_dir / graph_path.name
        data = torch.load(output_path)
        if hasattr(data, 'x') and data.x is not None:
            sample_features.append(data.x.numpy())

    if len(sample_features) > 0:
        sample_features = np.vstack(sample_features)
        sample_mean = np.mean(sample_features, axis=0).mean()
        sample_std = np.std(sample_features, axis=0).mean()

        print(f"Verification (from {sample_size} graphs):")
        print(f"  Normalized mean: {sample_mean:.6f} (should be ~0)")
        print(f"  Normalized std:  {sample_std:.6f} (should be ~1)")

    print(f"\n{'='*60}")
    print(f"Normalization Complete!")
    print(f"{'='*60}")
    print(f"\nOutput files:")
    print(f"  Normalized graphs: {output_dir}/ ({len(graph_files)} files)")
    print(f"  Normalization params: {params_output_path}")
    print(f"\nYou can now use these files for training.")
    print(f"The normalization parameters will be used during inference/testing.")

    return output_dir, params_output_path


def inspect_graphs(graph_dir, num_samples=10):
    """
    Inspect graph files and print basic statistics.

    Args:
        graph_dir: Directory containing .pt graph files
        num_samples: Number of graphs to sample for statistics
    """
    graph_dir = Path(graph_dir)

    if not graph_dir.exists():
        raise FileNotFoundError(f"Graph directory not found: {graph_dir}")

    graph_files = sorted(graph_dir.glob("*.pt"))

    if len(graph_files) == 0:
        raise ValueError(f"No .pt graph files found in {graph_dir}")

    print(f"\n{'='*60}")
    print(f"Inspecting Graph Files: {graph_dir}")
    print(f"{'='*60}\n")
    print(f"Total graph files: {len(graph_files)}")

    # Sample some graphs
    sample_size = min(num_samples, len(graph_files))
    sample_features = []
    sample_info = []

    print(f"\nSampling {sample_size} graphs for statistics...")

    for graph_path in tqdm(graph_files[:sample_size], desc="Loading samples"):
        try:
            data = torch.load(graph_path)

            info = {
                'file': graph_path.name,
                'num_nodes': data.x.shape[0] if hasattr(data, 'x') else 0,
                'feature_dim': data.x.shape[1] if hasattr(data, 'x') and len(data.x.shape) > 1 else 0,
                'num_edges': data.edge_index.shape[1] if hasattr(data, 'edge_index') else 0,
            }
            sample_info.append(info)

            if hasattr(data, 'x') and data.x is not None:
                sample_features.append(data.x.numpy())

        except Exception as e:
            print(f"Warning: Failed to load {graph_path}: {e}")

    if len(sample_features) > 0:
        # Concatenate features
        all_features = np.vstack(sample_features)

        print(f"\nGraph structure (from {len(sample_info)} samples):")
        avg_nodes = np.mean([info['num_nodes'] for info in sample_info])
        avg_edges = np.mean([info['num_edges'] for info in sample_info])
        feature_dim = sample_info[0]['feature_dim'] if sample_info else 0

        print(f"  Average nodes per graph: {avg_nodes:.1f}")
        print(f"  Average edges per graph: {avg_edges:.1f}")
        print(f"  Feature dimension: {feature_dim}")

        # Compute statistics
        mean = np.mean(all_features)
        std = np.std(all_features)
        min_val = np.min(all_features)
        max_val = np.max(all_features)

        print(f"\nFeature statistics (from {sample_size} graphs):")
        print(f"  Mean: {mean:.6f}")
        print(f"  Std:  {std:.6f}")
        print(f"  Min:  {min_val:.6f}")
        print(f"  Max:  {max_val:.6f}")

        # Check if data appears normalized
        is_normalized = abs(mean) < 0.1 and abs(std - 1.0) < 0.2
        print(f"\nAppears normalized: {'✓ Yes' if is_normalized else '✗ No'}")

        # Show some example files
        print(f"\nExample graph files:")
        for i, info in enumerate(sample_info[:5]):
            print(f"  {i+1}. {info['file']} ({info['num_nodes']} nodes, {info['num_edges']} edges)")
        if len(sample_info) > 5:
            print(f"  ... and {len(sample_info) - 5} more in sample")

    print(f"\n{'='*60}\n")


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description="Normalize existing graph node features",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Normalize and create new directory (default)
  python scripts/normalize_graphs.py --graph-dir data/processed/graphs

  # Normalize and specify output directory
  python scripts/normalize_graphs.py --graph-dir data/processed/graphs --output-dir data/processed/graphs_norm

  # Normalize in-place (overwrite original files)
  python scripts/normalize_graphs.py --graph-dir data/processed/graphs --inplace

  # Just inspect without normalizing
  python scripts/normalize_graphs.py --graph-dir data/processed/graphs --inspect-only

  # Specify custom location for normalization parameters
  python scripts/normalize_graphs.py --graph-dir data/processed/graphs --params-output params.npz
        """
    )

    parser.add_argument("--graph-dir", type=str, required=True,
                        help="Directory containing .pt graph files")
    parser.add_argument("--output-dir", type=str, default=None,
                        help="Directory to save normalized graphs (default: graph_dir_normalized)")
    parser.add_argument("--params-output", type=str, default=None,
                        help="Path to save normalization parameters (default: node_feature_norm_params.npz)")
    parser.add_argument("--inplace", action="store_true",
                        help="Overwrite original files instead of creating new directory")
    parser.add_argument("--inspect-only", action="store_true",
                        help="Only inspect the graphs, don't normalize")
    parser.add_argument("--num-samples", type=int, default=10,
                        help="Number of graphs to sample for inspection (default: 10)")

    args = parser.parse_args()

    try:
        if args.inspect_only:
            # Just inspect the graphs
            inspect_graphs(args.graph_dir, num_samples=args.num_samples)
        else:
            # Normalize the graphs
            output_dir, params_path = normalize_graph_features(
                graph_dir=args.graph_dir,
                output_dir=args.output_dir,
                params_output_path=args.params_output,
                inplace=args.inplace
            )

            # Optionally inspect the normalized output
            print("\n" + "="*60)
            print("Verifying normalized output...")
            print("="*60)
            inspect_graphs(output_dir, num_samples=args.num_samples)

    except Exception as e:
        print(f"\n❌ Error: {e}")
        import traceback
        traceback.print_exc()
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
