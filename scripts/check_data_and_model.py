#!/usr/bin/env python3
"""
å…¨é¢æ£€æŸ¥è¾“å…¥æ•°æ®å’Œæ¨¡å‹è¾“å‡º
"""
import sys
from pathlib import Path
import torch
import torch.nn.functional as F
import json
import h5py
import numpy as np
from torch_geometric.loader import DataLoader

sys.path.insert(0, str(Path(__file__).parent.parent))
from scripts.amber_vocabulary import get_global_encoder

# ç®€å•Dataset
class SimpleDataset(torch.utils.data.Dataset):
    def __init__(self, ids, graph_dir, emb_path):
        self.ids = []
        self.embs = {}
        with h5py.File(emb_path, 'r') as f:
            for cid in ids:
                base_id = cid.split('_model')[0] if '_model' in cid else cid
                graph_path = Path(graph_dir) / f'{cid}.pt'
                if graph_path.exists() and base_id in f:
                    self.ids.append(cid)
                    self.embs[cid] = torch.tensor(f[base_id][:], dtype=torch.float32)

    def __len__(self):
        return len(self.ids)

    def __getitem__(self, idx):
        cid = self.ids[idx]
        data = torch.load(Path('data/processed/graphs') / f'{cid}.pt', weights_only=False)
        data.y = self.embs[cid]
        return data

def check_tensor_stats(tensor, name, check_nan=True, check_inf=True, check_range=True):
    """æ£€æŸ¥tensorçš„ç»Ÿè®¡ä¿¡æ¯"""
    print(f"\n{name}:")
    print(f"  Shape: {tensor.shape}")
    print(f"  Dtype: {tensor.dtype}")
    print(f"  Device: {tensor.device}")
    print(f"  Mean: {tensor.mean().item():.6f}")
    print(f"  Std: {tensor.std().item():.6f}")
    print(f"  Min: {tensor.min().item():.6f}")
    print(f"  Max: {tensor.max().item():.6f}")
    print(f"  Norm: {tensor.norm().item():.6f}")

    issues = []

    if check_nan and torch.isnan(tensor).any():
        nan_count = torch.isnan(tensor).sum().item()
        print(f"  âŒ åŒ…å« {nan_count} ä¸ªNaNå€¼ï¼")
        issues.append(f"NaN ({nan_count}ä¸ª)")

    if check_inf and torch.isinf(tensor).any():
        inf_count = torch.isinf(tensor).sum().item()
        print(f"  âŒ åŒ…å« {inf_count} ä¸ªInfå€¼ï¼")
        issues.append(f"Inf ({inf_count}ä¸ª)")

    if check_range:
        if tensor.abs().max().item() > 1e6:
            print(f"  âš ï¸  å­˜åœ¨æå¤§å€¼ (>{1e6})")
            issues.append("æå¤§å€¼")
        if tensor.std().item() < 1e-6:
            print(f"  âš ï¸  æ ‡å‡†å·®æå° (<{1e-6})ï¼Œå‡ ä¹æ˜¯å¸¸æ•°")
            issues.append("è¿‘å¸¸æ•°")

    if not issues:
        print(f"  âœ“ æ•°å€¼æ­£å¸¸")

    return issues

print("="*80)
print("æ•°æ®å’Œæ¨¡å‹å…¨é¢æ£€æŸ¥")
print("="*80)

# Load data
with open('data/splits/splits.json', 'r') as f:
    splits = json.load(f)
train_ids = splits['train'][:10]  # ç”¨10ä¸ªæ ·æœ¬

dataset = SimpleDataset(train_ids, 'data/processed/graphs', 'data/processed/ligand_embeddings.h5')
loader = DataLoader(dataset, batch_size=2, shuffle=False)

print(f"\nDataset size: {len(dataset)}")

if len(dataset) == 0:
    print("âŒ æ²¡æœ‰å¯ç”¨æ•°æ®ï¼")
    sys.exit(1)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Device: {device}")

# ============================================================================
print("\n" + "="*80)
print("ç¬¬ä¸€éƒ¨åˆ†ï¼šæ£€æŸ¥è¾“å…¥æ•°æ®")
print("="*80)

print("\n1. æ£€æŸ¥Graphæ•°æ®ç»“æ„:")
batch = next(iter(loader))
print(f"  Batch size: {batch.num_graphs if hasattr(batch, 'num_graphs') else 'unknown'}")
print(f"  Total nodes: {batch.x.shape[0]}")
print(f"  Total edges: {batch.edge_index.shape[1]}")

# æ£€æŸ¥èŠ‚ç‚¹ç‰¹å¾
all_issues = []
issues = check_tensor_stats(batch.x, "  Node features (x)", check_nan=True, check_inf=True)
all_issues.extend(issues)

# æ£€æŸ¥åæ ‡
if hasattr(batch, 'pos'):
    issues = check_tensor_stats(batch.pos, "  Node positions (pos)", check_nan=True, check_inf=True)
    all_issues.extend(issues)
else:
    print("  âš ï¸  ç¼ºå°‘posï¼ˆèŠ‚ç‚¹åæ ‡ï¼‰")

# æ£€æŸ¥edge
print(f"\n  Edge index:")
print(f"    Shape: {batch.edge_index.shape}")
print(f"    Max node idx: {batch.edge_index.max().item()}")
print(f"    Min node idx: {batch.edge_index.min().item()}")

if batch.edge_index.min().item() < 0:
    print(f"    âŒ å­˜åœ¨è´Ÿæ•°ç´¢å¼•ï¼")
    all_issues.append("è´Ÿæ•°edgeç´¢å¼•")

if batch.edge_index.max().item() >= batch.x.shape[0]:
    print(f"    âŒ Edgeç´¢å¼•è¶…å‡ºèŠ‚ç‚¹æ•°é‡ï¼")
    all_issues.append("edgeç´¢å¼•è¶Šç•Œ")

# æ£€æŸ¥edge_attr
if hasattr(batch, 'edge_attr') and batch.edge_attr is not None:
    issues = check_tensor_stats(batch.edge_attr, "  Edge attributes", check_nan=True, check_inf=True)
    all_issues.extend(issues)

# æ£€æŸ¥multi-hop
if hasattr(batch, 'triple_index'):
    print(f"\n  Triple index (angles): {batch.triple_index.shape}")
    if hasattr(batch, 'triple_attr'):
        issues = check_tensor_stats(batch.triple_attr, "  Triple attributes", check_nan=True, check_inf=True)
        all_issues.extend(issues)

if hasattr(batch, 'nonbonded_edge_index'):
    print(f"\n  Nonbonded edges: {batch.nonbonded_edge_index.shape}")
    if hasattr(batch, 'nonbonded_edge_attr'):
        issues = check_tensor_stats(batch.nonbonded_edge_attr, "  Nonbonded attributes", check_nan=True, check_inf=True)
        all_issues.extend(issues)

# æ£€æŸ¥target embedding
print("\n2. æ£€æŸ¥Target Embeddings:")
target = batch.y
if target.dim() == 1:
    batch_size = batch.num_graphs if hasattr(batch, 'num_graphs') else 2
    target = target.view(batch_size, -1)

issues = check_tensor_stats(target, "  Target embeddings", check_nan=True, check_inf=True)
all_issues.extend(issues)

# æ£€æŸ¥targetçš„diversity
if target.shape[0] >= 2:
    from sklearn.metrics.pairwise import cosine_similarity
    target_np = target.cpu().numpy()
    cos_sim = cosine_similarity(target_np)
    mask = ~np.eye(cos_sim.shape[0], dtype=bool)

    print(f"\n  Target diversity (pairwise cosine similarity):")
    print(f"    Mean: {cos_sim[mask].mean():.6f}")
    print(f"    Std: {cos_sim[mask].std():.6f}")
    print(f"    Min: {cos_sim[mask].min():.6f}")
    print(f"    Max: {cos_sim[mask].max():.6f}")

    if cos_sim[mask].mean() > 0.8:
        print(f"    âš ï¸  Target embeddingså¤ªç›¸ä¼¼ï¼ˆå¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦>0.8ï¼‰")
        all_issues.append("Targetç¼ºä¹diversity")

# ============================================================================
print("\n" + "="*80)
print("ç¬¬äºŒéƒ¨åˆ†ï¼šæ£€æŸ¥æ¨¡å‹å’Œå‰å‘ä¼ æ’­")
print("="*80)

encoder = get_global_encoder()
from models.e3_gnn_encoder_v2 import RNAPocketEncoderV2

model = RNAPocketEncoderV2(
    num_atom_types=encoder.num_atom_types,
    num_residues=encoder.num_residues,
    atom_embed_dim=32,
    residue_embed_dim=16,
    hidden_irreps='32x0e + 16x1o + 8x2e',
    output_dim=1536,
    num_layers=6,
    num_radial_basis=8,
    use_multi_hop=True,
    use_nonbonded=True,
    use_gate=True,
    use_layer_norm=False,
    pooling_type='attention',
    dropout=0.0
).to(device)

print("\n3. æ£€æŸ¥æ¨¡å‹åˆå§‹åŒ–:")
print(f"  Total parameters: {sum(p.numel() for p in model.parameters()):,}")
print(f"  Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")

# æ£€æŸ¥å¯å­¦ä¹ æƒé‡
if hasattr(model, 'get_angle_weight'):
    aw = model.get_angle_weight().item()
    dw = model.get_dihedral_weight().item()
    nw = model.get_nonbonded_weight().item()
    print(f"\n  Learnable weights:")
    print(f"    angle_weight: {aw:.6f}")
    print(f"    dihedral_weight: {dw:.6f}")
    print(f"    nonbonded_weight: {nw:.6f}")

    if np.isnan(aw) or np.isnan(dw) or np.isnan(nw):
        print(f"    âŒ æƒé‡åˆå§‹åŒ–ä¸ºNaNï¼")
        all_issues.append("æƒé‡åˆå§‹åŒ–NaN")

# æ£€æŸ¥æ¨¡å‹å‚æ•°æ˜¯å¦æœ‰NaN
nan_params = []
for name, param in model.named_parameters():
    if torch.isnan(param).any():
        nan_params.append(name)

if nan_params:
    print(f"\n  âŒ å‘ç° {len(nan_params)} ä¸ªå‚æ•°åŒ…å«NaN:")
    for name in nan_params[:5]:
        print(f"    - {name}")
    all_issues.append("å‚æ•°åŒ…å«NaN")
else:
    print(f"  âœ“ æ‰€æœ‰å‚æ•°åˆå§‹åŒ–æ­£å¸¸")

# ============================================================================
print("\n4. æ£€æŸ¥å‰å‘ä¼ æ’­:")

batch = batch.to(device)
model.eval()

with torch.no_grad():
    try:
        output = model(batch)

        # æ£€æŸ¥è¾“å‡º
        issues = check_tensor_stats(output, "  Model output", check_nan=True, check_inf=True)
        all_issues.extend(issues)

        # æ£€æŸ¥è¾“å‡ºå½¢çŠ¶
        expected_shape = (batch.num_graphs if hasattr(batch, 'num_graphs') else 2, 1536)
        if output.shape != expected_shape:
            print(f"  âš ï¸  è¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…ï¼šæœŸæœ›{expected_shape}ï¼Œå®é™…{output.shape}")
            all_issues.append("è¾“å‡ºå½¢çŠ¶é”™è¯¯")

        # è®¡ç®—ä¸targetçš„ç›¸ä¼¼åº¦
        target = batch.y
        if target.dim() == 1:
            batch_size = batch.num_graphs if hasattr(batch, 'num_graphs') else output.size(0)
            target = target.view(batch_size, -1)

        cosine_sim = F.cosine_similarity(output, target, dim=1)
        print(f"\n  åˆå§‹æ€§èƒ½:")
        print(f"    Cosine similarity: {cosine_sim.mean().item():.6f} Â± {cosine_sim.std().item():.6f}")
        print(f"    MSE: {F.mse_loss(output, target).item():.6f}")

        if cosine_sim.mean().item() > 0.5:
            print(f"    âš ï¸  æœªè®­ç»ƒæ¨¡å‹çš„ä½™å¼¦ç›¸ä¼¼åº¦å¼‚å¸¸é«˜ (>0.5)")
            all_issues.append("åˆå§‹ç›¸ä¼¼åº¦å¼‚å¸¸")

    except Exception as e:
        print(f"  âŒ å‰å‘ä¼ æ’­å‡ºé”™: {e}")
        all_issues.append(f"å‰å‘ä¼ æ’­é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()

# ============================================================================
print("\n" + "="*80)
print("ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ£€æŸ¥æ¢¯åº¦è®¡ç®—")
print("="*80)

print("\n5. æ£€æŸ¥åå‘ä¼ æ’­:")

model.train()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

try:
    # Forward
    output = model(batch)
    target = batch.y
    if target.dim() == 1:
        batch_size = batch.num_graphs if hasattr(batch, 'num_graphs') else output.size(0)
        target = target.view(batch_size, -1)

    # Compute loss
    cosine_sim = F.cosine_similarity(output, target, dim=1)
    loss = (1 - cosine_sim).mean()

    print(f"  Loss: {loss.item():.6f}")

    if torch.isnan(loss):
        print(f"  âŒ Lossæ˜¯NaNï¼")
        all_issues.append("Lossæ˜¯NaN")

    # Backward
    optimizer.zero_grad()
    loss.backward()

    # æ£€æŸ¥æ¢¯åº¦
    nan_grads = []
    zero_grads = []
    total_grad_norm = 0
    max_grad = 0

    for name, param in model.named_parameters():
        if param.grad is not None:
            grad_norm = param.grad.norm().item()
            total_grad_norm += grad_norm ** 2
            max_grad = max(max_grad, param.grad.abs().max().item())

            if torch.isnan(param.grad).any():
                nan_grads.append(name)
            elif grad_norm < 1e-10:
                zero_grads.append(name)
        else:
            zero_grads.append(name + " (no grad)")

    total_grad_norm = total_grad_norm ** 0.5

    print(f"\n  æ¢¯åº¦ç»Ÿè®¡:")
    print(f"    Total gradient norm: {total_grad_norm:.6f}")
    print(f"    Max gradient: {max_grad:.6f}")
    print(f"    Parameters with gradients: {len([p for p in model.parameters() if p.grad is not None])}")
    print(f"    Parameters without gradients: {len([p for p in model.parameters() if p.grad is None])}")

    if nan_grads:
        print(f"\n  âŒ å‘ç° {len(nan_grads)} ä¸ªå‚æ•°çš„æ¢¯åº¦æ˜¯NaN:")
        for name in nan_grads[:5]:
            print(f"    - {name}")
        all_issues.append(f"{len(nan_grads)}ä¸ªNaNæ¢¯åº¦")

    if torch.isnan(torch.tensor(total_grad_norm)):
        print(f"  âŒ æ€»æ¢¯åº¦èŒƒæ•°æ˜¯NaNï¼")
        all_issues.append("æ€»æ¢¯åº¦NaN")
    elif total_grad_norm < 1e-6:
        print(f"  âš ï¸  æ¢¯åº¦èŒƒæ•°æå° (<1e-6)ï¼Œå¯èƒ½æ˜¯æ¢¯åº¦æ¶ˆå¤±")
        all_issues.append("æ¢¯åº¦æ¶ˆå¤±")
    elif total_grad_norm > 1e6:
        print(f"  âš ï¸  æ¢¯åº¦èŒƒæ•°æå¤§ (>1e6)ï¼Œå¯èƒ½æ˜¯æ¢¯åº¦çˆ†ç‚¸")
        all_issues.append("æ¢¯åº¦çˆ†ç‚¸")
    else:
        print(f"  âœ“ æ¢¯åº¦èŒƒæ•°æ­£å¸¸")

    if len(zero_grads) > 10:
        print(f"\n  âš ï¸  æœ‰ {len(zero_grads)} ä¸ªå‚æ•°çš„æ¢¯åº¦ä¸º0æˆ–ä¸å­˜åœ¨")
        print(f"    å‰5ä¸ª: {zero_grads[:5]}")

except Exception as e:
    print(f"  âŒ åå‘ä¼ æ’­å‡ºé”™: {e}")
    all_issues.append(f"åå‘ä¼ æ’­é”™è¯¯: {str(e)}")
    import traceback
    traceback.print_exc()

# ============================================================================
print("\n" + "="*80)
print("æ€»ç»“")
print("="*80)

if all_issues:
    print(f"\nâŒ å‘ç° {len(all_issues)} ä¸ªé—®é¢˜:")
    for i, issue in enumerate(all_issues, 1):
        print(f"  {i}. {issue}")

    print(f"\nğŸ”§ å»ºè®®:")
    if "NaN" in str(all_issues):
        print("  - æ£€æŸ¥æ•°æ®é¢„å¤„ç†æ˜¯å¦æ­£ç¡®")
        print("  - æ£€æŸ¥æ¨¡å‹åˆå§‹åŒ–")
        print("  - è¿è¡Œ python scripts/debug_exact_nan_location.py ç²¾ç¡®å®šä½")
    if "diversity" in str(all_issues).lower():
        print("  - æ£€æŸ¥target embeddingsæ˜¯å¦æ­£ç¡®åŠ è½½")
        print("  - æ£€æŸ¥embeddingsæ˜¯å¦è¢«è¿‡åº¦å½’ä¸€åŒ–")
    if "æ¢¯åº¦" in str(all_issues):
        print("  - å°è¯•å‡å°‘ç½‘ç»œå±‚æ•° (--num_layers 3)")
        print("  - å°è¯•å¢å¤§å­¦ä¹ ç‡ (--lr 1e-3)")
        print("  - å°è¯•ä½¿ç”¨LayerNorm (--use_layer_norm)")
else:
    print("\nâœ… æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼æ•°æ®å’Œæ¨¡å‹çœ‹èµ·æ¥æ­£å¸¸ã€‚")
    print("\nå¯ä»¥å¼€å§‹è®­ç»ƒ:")
    print("  bash test_nan_fix.sh")

print("\n" + "="*80)
