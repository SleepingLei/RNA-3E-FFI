#!/usr/bin/env python3
"""
è®­ç»ƒè„šæœ¬ç¤ºä¾‹ï¼šä½¿ç”¨ç‰©ç†çº¦æŸloss

å±•ç¤ºå¦‚ä½•åœ¨MSE lossåŸºç¡€ä¸Šæ·»åŠ ç‰©ç†çº¦æŸæ­£åˆ™åŒ–
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch_geometric.loader import DataLoader as GeometricDataLoader
import sys
from pathlib import Path

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from models.e3_gnn_encoder_v3 import RNAPocketEncoderV3
from models.improved_components import PhysicsConstraintLoss


def train_epoch_with_physics(
    model,
    loader,
    optimizer,
    device,
    target_embeddings,  # ç›®æ ‡embedding (é…ä½“)
    physics_loss_fn,
    physics_weight=0.1,  # ç‰©ç†lossçš„æƒé‡
    grad_clip=0.0
):
    """
    è®­ç»ƒä¸€ä¸ªepochï¼Œä½¿ç”¨MSE loss + ç‰©ç†çº¦æŸloss

    Args:
        model: RNAPocketEncoderV3æ¨¡å‹
        loader: DataLoader
        optimizer: ä¼˜åŒ–å™¨
        device: è®¾å¤‡
        target_embeddings: dict, {sample_id: target_embedding}
        physics_loss_fn: PhysicsConstraintLoss
        physics_weight: ç‰©ç†lossçš„æƒé‡ç³»æ•°
        grad_clip: æ¢¯åº¦è£å‰ªé˜ˆå€¼

    Returns:
        epoch_metrics: dict with losses
    """
    model.train()

    total_mse_loss = 0.0
    total_physics_loss = 0.0
    total_combined_loss = 0.0

    physics_components = {
        'bond_energy': 0.0,
        'angle_energy': 0.0,
        'dihedral_energy': 0.0
    }

    num_batches = 0

    for batch_idx, data in enumerate(loader):
        data = data.to(device)

        # Forward pass
        pocket_embedding = model(data)  # [batch_size, 512]

        # ========== 1. ä¸»ä»»åŠ¡loss (MSE) ==========
        # è·å–target embeddings
        batch_size = pocket_embedding.size(0)
        target_emb_list = []
        for i in range(batch_size):
            # å‡è®¾dataä¸­æœ‰sample_id
            sample_id = data.sample_id[i] if hasattr(data, 'sample_id') else i
            target_emb = target_embeddings.get(sample_id, torch.zeros(512, device=device))
            target_emb_list.append(target_emb)

        target_emb = torch.stack(target_emb_list)  # [batch_size, 512]

        # MSE loss
        mse_loss = nn.MSELoss()(pocket_embedding, target_emb)

        # ========== 2. ç‰©ç†çº¦æŸloss (æ­£åˆ™åŒ–) ==========
        # æ³¨æ„: ç‰©ç†lossä½¿ç”¨data.posï¼Œä¸éœ€è¦æ¢¯åº¦
        # æˆ‘ä»¬åªæ˜¯ç”¨å®ƒæ¥çº¦æŸå­¦ä¹ çš„embeddingæ›´ç¬¦åˆç‰©ç†
        physics_loss, physics_dict = physics_loss_fn(data)

        # ========== 3. ç»„åˆloss ==========
        combined_loss = mse_loss + physics_weight * physics_loss

        # Backward
        optimizer.zero_grad()
        combined_loss.backward()

        # Gradient clipping
        if grad_clip > 0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)

        optimizer.step()

        # è®°å½•
        total_mse_loss += mse_loss.item()
        total_physics_loss += physics_loss.item()
        total_combined_loss += combined_loss.item()

        for key in physics_components:
            if key in physics_dict:
                physics_components[key] += physics_dict[key]

        num_batches += 1

        if batch_idx % 10 == 0:
            print(f"Batch {batch_idx}/{len(loader)}: "
                  f"MSE={mse_loss.item():.4f}, "
                  f"Physics={physics_loss.item():.4f}, "
                  f"Total={combined_loss.item():.4f}")

    # è®¡ç®—å¹³å‡
    epoch_metrics = {
        'mse_loss': total_mse_loss / num_batches,
        'physics_loss': total_physics_loss / num_batches,
        'combined_loss': total_combined_loss / num_batches,
    }

    for key in physics_components:
        epoch_metrics[key] = physics_components[key] / num_batches

    return epoch_metrics


def main():
    """ä¸»è®­ç»ƒæµç¨‹ç¤ºä¾‹"""

    # ========== é…ç½® ==========
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    # ========== åˆ›å»ºæ¨¡å‹ ==========
    print("\n1. Creating model...")
    model = RNAPocketEncoderV3(
        output_dim=512,
        num_layers=4,
        use_geometric_mp=True,        # âœ… ä½¿ç”¨å‡ ä½•å¢å¼ºMP
        use_enhanced_invariants=True,  # âœ… ä½¿ç”¨å¢å¼ºä¸å˜é‡
        pooling_type='multihead_attention',  # âœ… ä½¿ç”¨å¤šå¤´æ³¨æ„åŠ›
        num_attention_heads=4,
        dropout=0.1
    ).to(device)

    print(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")

    # ========== åˆ›å»ºç‰©ç†çº¦æŸloss ==========
    print("\n2. Creating physics constraint loss...")
    physics_loss_fn = PhysicsConstraintLoss(
        use_bond=True,       # âœ… é”®ä¼¸ç¼©èƒ½é‡
        use_angle=True,      # âœ… é”®è§’å¼¯æ›²èƒ½é‡
        use_dihedral=True,   # âœ… äºŒé¢è§’æ‰­è½¬èƒ½é‡
        use_nonbonded=False, # âŒ éé”®èƒ½é‡ï¼ˆé€šå¸¸ä¸ç”¨ï¼Œè®¡ç®—é‡å¤§ï¼‰
        reduction='mean'
    )

    # ========== ä¼˜åŒ–å™¨ ==========
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=1e-4,
        weight_decay=1e-5
    )

    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer,
        mode='min',
        factor=0.5,
        patience=5,
        verbose=True
    )

    # ========== åŠ è½½æ•°æ® (ç¤ºä¾‹) ==========
    print("\n3. Loading data...")
    # è¿™é‡Œåº”è¯¥åŠ è½½ä½ çš„å®é™…æ•°æ®
    # train_loader = GeometricDataLoader(train_dataset, batch_size=32, shuffle=True)
    # val_loader = GeometricDataLoader(val_dataset, batch_size=32)

    # ç¤ºä¾‹ï¼šå‡è®¾target_embeddings
    target_embeddings = {}
    # åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œè¿™åº”è¯¥æ˜¯é…ä½“çš„embedding
    # target_embeddings[sample_id] = ligand_embedding

    # ========== è®­ç»ƒå¾ªç¯ ==========
    print("\n4. Training...")
    num_epochs = 100
    best_val_loss = float('inf')

    # ç‰©ç†lossæƒé‡ç­–ç•¥
    physics_weight_schedule = {
        0: 0.01,   # åˆæœŸå¾ˆå°
        10: 0.05,  # é€æ¸å¢å¤§
        20: 0.1,   # ç¨³å®šå€¼
    }

    for epoch in range(num_epochs):
        # åŠ¨æ€è°ƒæ•´physics lossæƒé‡
        physics_weight = physics_weight_schedule.get(
            epoch,
            physics_weight_schedule[max([k for k in physics_weight_schedule.keys() if k <= epoch])]
        )

        print(f"\n{'='*80}")
        print(f"Epoch {epoch+1}/{num_epochs}")
        print(f"Physics loss weight: {physics_weight:.4f}")
        print(f"{'='*80}")

        # è®­ç»ƒ
        # train_metrics = train_epoch_with_physics(
        #     model=model,
        #     loader=train_loader,
        #     optimizer=optimizer,
        #     device=device,
        #     target_embeddings=target_embeddings,
        #     physics_loss_fn=physics_loss_fn,
        #     physics_weight=physics_weight,
        #     grad_clip=1.0
        # )

        # print(f"\nTraining Metrics:")
        # print(f"  MSE Loss:      {train_metrics['mse_loss']:.4f}")
        # print(f"  Physics Loss:  {train_metrics['physics_loss']:.4f}")
        # print(f"  Combined Loss: {train_metrics['combined_loss']:.4f}")
        # print(f"  - Bond Energy:     {train_metrics['bond_energy']:.4f}")
        # print(f"  - Angle Energy:    {train_metrics['angle_energy']:.4f}")
        # print(f"  - Dihedral Energy: {train_metrics['dihedral_energy']:.4f}")

        # éªŒè¯ (ä¸ä½¿ç”¨physics loss)
        # val_loss = evaluate(model, val_loader, device, target_embeddings)
        # print(f"\nValidation MSE Loss: {val_loss:.4f}")

        # # å­¦ä¹ ç‡è°ƒåº¦
        # scheduler.step(val_loss)

        # # ä¿å­˜æœ€ä½³æ¨¡å‹
        # if val_loss < best_val_loss:
        #     best_val_loss = val_loss
        #     torch.save({
        #         'epoch': epoch,
        #         'model_state_dict': model.state_dict(),
        #         'optimizer_state_dict': optimizer.state_dict(),
        #         'val_loss': val_loss,
        #     }, 'best_model_v3.pt')
        #     print(f"âœ“ Saved best model (val_loss: {val_loss:.4f})")

    print("\n" + "="*80)
    print("Training completed!")
    print("="*80)


def evaluate(model, loader, device, target_embeddings):
    """éªŒè¯å‡½æ•°ï¼ˆä¸ä½¿ç”¨physics lossï¼‰"""
    model.eval()
    total_loss = 0.0
    num_batches = 0

    with torch.no_grad():
        for data in loader:
            data = data.to(device)

            pocket_embedding = model(data)

            # è·å–target
            batch_size = pocket_embedding.size(0)
            target_emb_list = []
            for i in range(batch_size):
                sample_id = data.sample_id[i] if hasattr(data, 'sample_id') else i
                target_emb = target_embeddings.get(sample_id, torch.zeros(512, device=device))
                target_emb_list.append(target_emb)

            target_emb = torch.stack(target_emb_list)

            loss = nn.MSELoss()(pocket_embedding, target_emb)
            total_loss += loss.item()
            num_batches += 1

    return total_loss / num_batches


# ============================================================================
# ä½¿ç”¨è¯´æ˜å’Œæœ€ä½³å®è·µ
# ============================================================================

"""
ğŸ“– ä½¿ç”¨æŒ‡å—

1. **ç‰©ç†lossæƒé‡è°ƒä¼˜**:
   - åˆæœŸ (epoch 0-10): 0.01-0.05 (è®©æ¨¡å‹å…ˆå­¦ä¸»ä»»åŠ¡)
   - ä¸­æœŸ (epoch 10-20): 0.05-0.1 (é€æ¸å¢åŠ çº¦æŸ)
   - åæœŸ (epoch 20+): 0.1-0.2 (ç¨³å®šæ­£åˆ™åŒ–)

   å¦‚æœphysics_loss >> mse_loss:
   - å‡å°physics_weight
   - æˆ–è€…å¯¹physics_losså„é¡¹åŠ æƒ

2. **ä½•æ—¶ä½¿ç”¨ç‰©ç†loss**:
   âœ… é€‚ç”¨åœºæ™¯:
   - è®­ç»ƒæ•°æ®è¾ƒå°‘æ—¶ï¼ˆå¸®åŠ©æ³›åŒ–ï¼‰
   - æ¨¡å‹å€¾å‘è¿‡æ‹Ÿåˆæ—¶
   - éœ€è¦ç‰©ç†å¯è§£é‡Šæ€§æ—¶

   âŒ ä¸é€‚ç”¨:
   - æ•°æ®å……è¶³ä¸”è´¨é‡é«˜
   - ä¸»ä»»åŠ¡losså·²ç»å¾ˆå¥½æ”¶æ•›

3. **ç›‘æ§æŒ‡æ ‡**:
   - å¦‚æœphysics_lossæŒç»­ä¸‹é™ï¼Œè¯´æ˜æ¨¡å‹å­¦åˆ°äº†ç‰©ç†è§„å¾‹ âœ“
   - å¦‚æœphysics_lossä¸Šå‡ä½†mse_lossä¸‹é™ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´æƒé‡
   - è§‚å¯ŸéªŒè¯é›†æ€§èƒ½æ¥åˆ¤æ–­æ˜¯å¦è¿‡åº¦æ­£åˆ™åŒ–

4. **å˜ä½“ç­–ç•¥**:

   Strategy A - ä»…å‰æœŸä½¿ç”¨:
   ```python
   if epoch < 20:
       physics_weight = 0.1
   else:
       physics_weight = 0.0  # åæœŸå…³é—­
   ```

   Strategy B - åˆ†é¡¹åŠ æƒ:
   ```python
   physics_loss = (
       1.0 * bond_energy +      # é”®å¾ˆé‡è¦
       0.5 * angle_energy +     # è§’åº¦æ¬¡è¦
       0.3 * dihedral_energy    # äºŒé¢è§’æœ€ä¸é‡è¦
   )
   ```

   Strategy C - åªåœ¨ç‰¹å®šæ ·æœ¬ä½¿ç”¨:
   ```python
   # å¯¹å¤§åˆ†å­ä½¿ç”¨æ›´å¼ºçš„ç‰©ç†çº¦æŸ
   if data.num_nodes > 100:
       physics_weight = 0.2
   else:
       physics_weight = 0.05
   ```

5. **è°ƒè¯•å»ºè®®**:
   ```python
   # æ‰“å°å„é¡¹èƒ½é‡ï¼Œç¡®ä¿æ•°å€¼åˆç†
   print(f"Bond energy: {bond_energy:.2f} kcal/mol")
   print(f"Angle energy: {angle_energy:.2f} kcal/mol")
   print(f"Dihedral energy: {dihedral_energy:.2f} kcal/mol")

   # å¦‚æœæŸé¡¹ç‰¹åˆ«å¤§ï¼Œæ£€æŸ¥:
   # - è¾“å…¥æ•°æ®çš„å•ä½æ˜¯å¦æ­£ç¡®
   # - åŠ›å¸¸æ•°æ˜¯å¦å¼‚å¸¸
   # - å¹³è¡¡å€¼æ˜¯å¦åˆç†
   ```

6. **æ€§èƒ½ä¼˜åŒ–**:
   - ç‰©ç†lossè®¡ç®—ä¸éœ€è¦åå‘ä¼ æ’­åˆ°data.pos
   - å¯ä»¥æ¯Nä¸ªbatchè®¡ç®—ä¸€æ¬¡physics loss
   ```python
   if batch_idx % 5 == 0:  # æ¯5ä¸ªbatchè®¡ç®—ä¸€æ¬¡
       physics_loss, _ = physics_loss_fn(data)
   ```
"""

if __name__ == "__main__":
    print(__doc__)
    print("\n" + "="*80)
    print("This is a template script. Modify it for your actual use case.")
    print("="*80)
    # Uncomment to run:
    # main()
